{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "__credits__ = [\"Carlos Luis\"]\n",
    "\n",
    "from os import path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "\n",
    "DEFAULT_X = np.pi\n",
    "DEFAULT_Y = 1.0\n",
    "\n",
    "\n",
    "class PendulumEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ## Description\n",
    "\n",
    "    The inverted pendulum swingup problem is based on the classic problem in control theory.\n",
    "    The system consists of a pendulum attached at one end to a fixed point, and the other end being free.\n",
    "    The pendulum starts in a random position and the goal is to apply torque on the free end to swing it\n",
    "    into an upright position, with its center of gravity right above the fixed point.\n",
    "\n",
    "    The diagram below specifies the coordinate system used for the implementation of the pendulum's\n",
    "    dynamic equations.\n",
    "\n",
    "    ![Pendulum Coordinate System](/_static/diagrams/pendulum.png)\n",
    "\n",
    "    - `x-y`: cartesian coordinates of the pendulum's end in meters.\n",
    "    - `theta` : angle in radians.\n",
    "    - `tau`: torque in `N m`. Defined as positive _counter-clockwise_.\n",
    "\n",
    "    ## Action Space\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` representing the torque applied to free end of the pendulum.\n",
    "\n",
    "    | Num | Action | Min  | Max |\n",
    "    |-----|--------|------|-----|\n",
    "    | 0   | Torque | -2.0 | 2.0 |\n",
    "\n",
    "    ## Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(3,)` representing the x-y coordinates of the pendulum's free\n",
    "    end and its angular velocity.\n",
    "\n",
    "    | Num | Observation      | Min  | Max |\n",
    "    |-----|------------------|------|-----|\n",
    "    | 0   | x = cos(theta)   | -1.0 | 1.0 |\n",
    "    | 1   | y = sin(theta)   | -1.0 | 1.0 |\n",
    "    | 2   | Angular Velocity | -8.0 | 8.0 |\n",
    "\n",
    "    ## Rewards\n",
    "\n",
    "    The reward function is defined as:\n",
    "\n",
    "    *r = -(theta<sup>2</sup> + 0.1 * theta_dt<sup>2</sup> + 0.001 * torque<sup>2</sup>)*\n",
    "\n",
    "    where `theta` is the pendulum's angle normalized between *[-pi, pi]* (with 0 being in the upright position).\n",
    "    Based on the above equation, the minimum reward that can be obtained is\n",
    "    *-(pi<sup>2</sup> + 0.1 * 8<sup>2</sup> + 0.001 * 2<sup>2</sup>) = -16.2736044*,\n",
    "    while the maximum reward is zero (pendulum is upright with zero velocity and no torque applied).\n",
    "\n",
    "    ## Starting State\n",
    "\n",
    "    The starting state is a random angle in *[-pi, pi]* and a random angular velocity in *[-1,1]*.\n",
    "\n",
    "    ## Episode Truncation\n",
    "\n",
    "    The episode truncates at 200 time steps.\n",
    "\n",
    "    ## Arguments\n",
    "\n",
    "    - `g`: .\n",
    "\n",
    "    Pendulum has two parameters for `gymnasium.make` with `render_mode` and `g` representing\n",
    "    the acceleration of gravity measured in *(m s<sup>-2</sup>)* used to calculate the pendulum dynamics.\n",
    "    The default value is `g = 10.0`.\n",
    "    On reset, the `options` parameter allows the user to change the bounds used to determine the new random state.\n",
    "\n",
    "    ```python\n",
    "    >>> import gymnasium as gym\n",
    "    >>> env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\", g=9.81)  # default g=10.0\n",
    "    >>> env\n",
    "    <TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>\n",
    "    >>> env.reset(seed=123, options={\"low\": -0.7, \"high\": 0.5})  # default low=-0.6, high=-0.5\n",
    "    (array([ 0.4123625 ,  0.91101986, -0.89235795], dtype=float32), {})\n",
    "\n",
    "    ```\n",
    "\n",
    "    ## Version History\n",
    "\n",
    "    * v1: Simplify the math equations, no difference in behavior.\n",
    "    * v0: Initial versions release\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, g=10.0):\n",
    "        self.max_speed = 8\n",
    "        self.max_torque = 2.0\n",
    "        self.dt = 0.05\n",
    "        self.g = g\n",
    "        self.m = 1.0\n",
    "        self.l = 1.0\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_dim = 500\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "\n",
    "        high = np.array([1.0, 1.0, self.max_speed], dtype=np.float32)\n",
    "        # This will throw a warning in tests/envs/test_envs in utils/env_checker.py as the space is not symmetric\n",
    "        #   or normalised as max_torque == 2 by default. Ignoring the issue here as the default settings are too old\n",
    "        #   to update to follow the gymnasium api\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-self.max_torque, high=self.max_torque, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "\n",
    "    def step(self, u):\n",
    "        th, thdot = self.state  # th := theta\n",
    "\n",
    "        g = self.g\n",
    "        m = self.m\n",
    "        l = self.l\n",
    "        dt = self.dt\n",
    "        \n",
    "        \n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)\n",
    "        self.last_u = u  # for rendering\n",
    "        costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n",
    "\n",
    "        newthdot = thdot + (3 * g / (2 * l) * np.sin(th) + 3.0 / (m * l**2) * u) * dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "        newth = th + newthdot * dt\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return self._get_obs(), -float(costs), False, False, {}\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        if options is None:\n",
    "            high = np.array([DEFAULT_X, DEFAULT_Y])\n",
    "        else:\n",
    "            # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "            # state/observations.\n",
    "            x = options.get(\"x_init\") if \"x_init\" in options else DEFAULT_X\n",
    "            y = options.get(\"y_init\") if \"y_init\" in options else DEFAULT_Y\n",
    "            x = utils.verify_number_and_cast(x)\n",
    "            y = utils.verify_number_and_cast(y)\n",
    "            high = np.array([x, y])\n",
    "        low = -high  # We enforce symmetric limits.\n",
    "        self.state = self.np_random.uniform(low=low, high=high)\n",
    "        self.last_u = None\n",
    "\n",
    "        # if self.render_mode == \"human\":\n",
    "        #     self.render()\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        theta, thetadot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), thetadot], \n",
    "                        dtype=np.float32).reshape(-1)\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            assert self.spec is not None\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gymnasium[classic-control]`\"\n",
    "            ) from e\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_dim, self.screen_dim)\n",
    "                )\n",
    "            else:  # mode in \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.screen_dim, self.screen_dim))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_dim, self.screen_dim))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        bound = 2.2\n",
    "        scale = self.screen_dim / (bound * 2)\n",
    "        offset = self.screen_dim // 2\n",
    "\n",
    "        rod_length = 1 * scale\n",
    "        rod_width = 0.2 * scale\n",
    "        l, r, t, b = 0, rod_length, rod_width / 2, -rod_width / 2\n",
    "        coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        transformed_coords = []\n",
    "        for c in coords:\n",
    "            c = pygame.math.Vector2(c).rotate_rad(self.state[0] + np.pi / 2)\n",
    "            c = (c[0] + offset, c[1] + offset)\n",
    "            transformed_coords.append(c)\n",
    "        gfxdraw.aapolygon(self.surf, transformed_coords, (204, 77, 77))\n",
    "        gfxdraw.filled_polygon(self.surf, transformed_coords, (204, 77, 77))\n",
    "\n",
    "        gfxdraw.aacircle(self.surf, offset, offset, int(rod_width / 2), (204, 77, 77))\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf, offset, offset, int(rod_width / 2), (204, 77, 77)\n",
    "        )\n",
    "\n",
    "        rod_end = (rod_length, 0)\n",
    "        rod_end = pygame.math.Vector2(rod_end).rotate_rad(self.state[0] + np.pi / 2)\n",
    "        rod_end = (int(rod_end[0] + offset), int(rod_end[1] + offset))\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf, rod_end[0], rod_end[1], int(rod_width / 2), (204, 77, 77)\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf, rod_end[0], rod_end[1], int(rod_width / 2), (204, 77, 77)\n",
    "        )\n",
    "\n",
    "        fname = path.join(path.dirname(__file__), \"assets/clockwise.png\")\n",
    "        img = pygame.image.load(fname)\n",
    "        if self.last_u is not None:\n",
    "            scale_img = pygame.transform.smoothscale(\n",
    "                img,\n",
    "                (scale * np.abs(self.last_u) / 2, scale * np.abs(self.last_u) / 2),\n",
    "            )\n",
    "            is_flip = bool(self.last_u > 0)\n",
    "            scale_img = pygame.transform.flip(scale_img, is_flip, True)\n",
    "            self.surf.blit(\n",
    "                scale_img,\n",
    "                (\n",
    "                    offset - scale_img.get_rect().centerx,\n",
    "                    offset - scale_img.get_rect().centery,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # drawing axle\n",
    "        gfxdraw.aacircle(self.surf, offset, offset, int(0.05 * scale), (0, 0, 0))\n",
    "        gfxdraw.filled_circle(self.surf, offset, offset, int(0.05 * scale), (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        else:  # mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egtechlab/anaconda3/envs/mujoco_env/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:441: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_85715/3941152309.py:150: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return self._get_obs(), -float(costs), False, False, {}\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = PendulumEnv()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "Box(-2.0, 2.0, (1,), float32)\n",
      "[-1.5355307]\n",
      "Step 1\n",
      "obs= [0.11507089 0.9933573  0.25004393] reward= -2.1253013610839844 done= False\n",
      "Step 2\n",
      "obs= [0.07448856 0.9972219  0.81537575] reward= -2.126079797744751 done= False\n",
      "Step 3\n",
      "obs= [0.00219567 0.9999976  1.4472382 ] reward= -2.3058133125305176 done= False\n",
      "Step 4\n",
      "obs= [-0.1065511  0.9943072  2.178989 ] reward= -2.669973373413086 done= False\n",
      "Step 5\n",
      "obs= [-0.24592909  0.9692878   2.8344865 ] reward= -3.2893354892730713 done= False\n",
      "Step 6\n",
      "obs= [-0.4150195  0.9098125  3.5897236] reward= -4.11322546005249 done= False\n",
      "Step 7\n",
      "obs= [-0.60624063  0.7952813   4.467214  ] reward= -5.285346984863281 done= False\n",
      "Step 8\n",
      "obs= [-0.7933436  0.6087741  5.2991405] reward= -6.935888290405273 done= False\n",
      "Step 9\n",
      "obs= [-0.9354829   0.35337186  5.866827  ] reward= -8.994194030761719 done= False\n",
      "Step 10\n",
      "obs= [-0.9977981   0.06632477  5.8959894 ] reward= -11.175171852111816 done= False\n",
      "Step 11\n",
      "obs= [-0.9758973 -0.2182302  5.727481 ] reward= -12.935359954833984 done= False\n",
      "Step 12\n",
      "obs= [-0.8749921  -0.48413724  5.7075276 ] reward= -11.817022323608398 done= False\n",
      "Step 13\n",
      "obs= [-0.72551596 -0.68820536  5.072714  ] reward= -10.210503578186035 done= False\n",
      "Step 14\n",
      "obs= [-0.55391884 -0.8325707   4.4944    ] reward= -8.250102043151855 done= False\n",
      "Step 15\n",
      "obs= [-0.37016279 -0.9289669   4.1575933 ] reward= -6.679999828338623 done= False\n",
      "Step 16\n",
      "obs= [-0.19138128 -0.98151577  3.7322965 ] reward= -5.5342583656311035 done= False\n",
      "Step 17\n",
      "obs= [-0.0560138 -0.99843    2.7305198] reward= -4.505599498748779 done= False\n",
      "Step 18\n",
      "obs= [ 0.03492504 -0.99938995  1.8195084 ] reward= -3.393350839614868 done= False\n",
      "Step 19\n",
      "obs= [ 0.08049592 -0.99675494  0.9130186 ] reward= -2.6910345554351807 done= False\n",
      "Step 20\n",
      "obs= [ 0.07475645 -0.9972018  -0.11513424] reward= -2.30759596824646 done= False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85715/3941152309.py:150: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return self._get_obs(), -float(costs), False, False, {}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "obs, _ = env.reset()\n",
    "# env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "# Hardcoded best agent: always go left!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    print(f\"Step {step + 1}\")\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    # env.render()\n",
    "    if done:\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85715/3941152309.py:150: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return self._get_obs(), -float(costs), False, False, {}\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN, SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "vec_env = make_vec_env(PendulumEnv, n_envs=1)\n",
    "\n",
    "# Train the agent\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1).learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 and action=[[1.9843888]]\n",
      "obs= [[-0.9981501  0.060798   0.2601578]] reward= [-9.415885] done= [False]\n",
      "Step 2 and action=[[1.9798634]]\n",
      "obs= [[-0.9995288   0.03069387  0.6027358 ]] reward= [-9.501754] done= [False]\n",
      "Step 3 and action=[[1.9799623]]\n",
      "obs= [[-0.99988085 -0.01543827  0.9227506 ]] reward= [-9.717911] done= [False]\n",
      "Step 4 and action=[[1.9814253]]\n",
      "obs= [[-0.99712414 -0.07578547  1.2083857 ]] reward= [-9.861913] done= [False]\n",
      "Step 5 and action=[[1.9813585]]\n",
      "obs= [[-0.98902434 -0.14775278  1.4487504 ]] reward= [-9.548674] done= [False]\n",
      "Step 6 and action=[[1.9797628]]\n",
      "obs= [[-0.97365713 -0.22801709  1.6349002 ]] reward= [-9.173634] done= [False]\n",
      "Step 7 and action=[[1.9780228]]\n",
      "obs= [[-0.94984066 -0.3127342   1.7605908 ]] reward= [-8.74834] done= [False]\n",
      "Step 8 and action=[[1.9763482]]\n",
      "obs= [[-0.9174415  -0.39787072  1.8224924 ]] reward= [-8.286156] done= [False]\n",
      "Step 9 and action=[[1.9710579]]\n",
      "obs= [[-0.87749517 -0.47958544  1.819748  ]] reward= [-7.8020334] done= [False]\n",
      "Step 10 and action=[[1.9620364]]\n",
      "obs= [[-0.83210695 -0.5546152   1.7543645 ]] reward= [-7.3120475] done= [False]\n",
      "Step 11 and action=[[1.950954]]\n",
      "obs= [[-0.7841614  -0.62055695  1.6310462 ]] reward= [-6.8329287] done= [False]\n",
      "Step 12 and action=[[1.9471846]]\n",
      "obs= [[-0.7368901  -0.67601264  1.4577061 ]] reward= [-6.381299] done= [False]\n",
      "Step 13 and action=[[1.9512138]]\n",
      "obs= [[-0.6934666 -0.7204887  1.2433788]] reward= [-5.9727225] done= [False]\n",
      "Step 14 and action=[[1.9499521]]\n",
      "obs= [[-0.65676    -0.7540997   0.99550503]] reward= [-5.6203723] done= [False]\n",
      "Step 15 and action=[[1.9211752]]\n",
      "obs= [[-0.62926626 -0.7771898   0.7181065 ]] reward= [-5.3345847] done= [False]\n",
      "Step 16 and action=[[1.5020139]]\n",
      "obs= [[-0.6151553  -0.788406    0.36051625]] reward= [-5.122648] done= [False]\n",
      "Step 17 and action=[[-1.7119261]]\n",
      "obs= [[-0.6341909 -0.7731765 -0.4875772]] reward= [-5.0039124] done= [False]\n",
      "Step 18 and action=[[-1.9676574]]\n",
      "obs= [[-0.6853558  -0.72820836 -1.3626082 ]] reward= [-5.1251173] done= [False]\n",
      "Step 19 and action=[[-1.979396]]\n",
      "obs= [[-0.76133895 -0.64835405 -2.205674  ]] reward= [-5.599347] done= [False]\n",
      "Step 20 and action=[[-1.9803572]]\n",
      "obs= [[-0.84938836 -0.52776825 -2.9889932 ]] reward= [-6.425357] done= [False]\n",
      "Step 21 and action=[[-1.987715]]\n",
      "obs= [[-0.93166685 -0.36331373 -3.6829767 ]] reward= [-7.5828004] done= [False]\n",
      "Step 22 and action=[[-1.9901515]]\n",
      "obs= [[-0.9873666  -0.15845238 -4.253985  ]] reward= [-9.032022] done= [False]\n",
      "Step 23 and action=[[-1.987314]]\n",
      "obs= [[-0.9972319   0.07435431 -4.6709213 ]] reward= [-10.7087145] done= [False]\n",
      "Step 24 and action=[[-1.9869024]]\n",
      "obs= [[-0.94920945  0.31464493 -4.913191  ]] reward= [-11.593227] done= [False]\n",
      "Step 25 and action=[[-1.9867573]]\n",
      "obs= [[-0.84252447  0.5386581  -4.9752207 ]] reward= [-10.378811] done= [False]\n",
      "Step 26 and action=[[-1.9870554]]\n",
      "obs= [[-0.68782514  0.72587645 -4.8692856 ]] reward= [-9.09827] done= [False]\n",
      "Step 27 and action=[[-1.9897716]]\n",
      "obs= [[-0.5032205   0.86415803 -4.623344  ]] reward= [-7.8005214] done= [False]\n",
      "Step 28 and action=[[-1.9912668]]\n",
      "obs= [[-0.30850953  0.9512212  -4.2739153 ]] reward= [-6.5435963] done= [False]\n",
      "Step 29 and action=[[-1.9895953]]\n",
      "obs= [[-0.12038612  0.99272716 -3.8589387 ]] reward= [-5.3816404] done= [False]\n",
      "Step 30 and action=[[-1.9891282]]\n",
      "obs= [[ 0.04993848  0.9987523  -3.4127626 ]] reward= [-4.354186] done= [False]\n",
      "Step 31 and action=[[-1.9902278]]\n",
      "obs= [[ 0.19677825  0.980448   -2.9622326 ]] reward= [-3.4816012] done= [False]\n",
      "Step 32 and action=[[-1.9912134]]\n",
      "obs= [[ 0.3186925   0.94785815 -2.5255785 ]] reward= [-2.7658231] done= [False]\n",
      "Step 33 and action=[[-1.9917316]]\n",
      "obs= [[ 0.41689077  0.9089566  -2.1134446 ]] reward= [-2.1954515] done= [False]\n",
      "Step 34 and action=[[-1.9869338]]\n",
      "obs= [[ 0.49384868  0.86954784 -1.7297672 ]] reward= [-1.7519791] done= [False]\n",
      "Step 35 and action=[[-1.9821033]]\n",
      "obs= [[ 0.5524131  0.8335705 -1.3749218]] reward= [-1.4146577] done= [False]\n",
      "Step 36 and action=[[-1.9775355]]\n",
      "obs= [[ 0.5952487   0.80354154 -1.0463743 ]] reward= [-1.1642401] done= [False]\n",
      "Step 37 and action=[[-1.9793417]]\n",
      "obs= [[ 0.62458974  0.78095305 -0.7406194 ]] reward= [-0.9843094] done= [False]\n",
      "Step 38 and action=[[-1.9798732]]\n",
      "obs= [[ 0.64207387  0.76664275 -0.4518856 ]] reward= [-0.8619289] done= [False]\n",
      "Step 39 and action=[[-1.9787453]]\n",
      "obs= [[ 0.64870846  0.761037   -0.17371532]] reward= [-0.7875054] done= [False]\n",
      "Step 40 and action=[[-1.9726539]]\n",
      "obs= [[0.6448507  0.7643086  0.10116434]] reward= [-0.754979] done= [False]\n",
      "Step 41 and action=[[-1.9517393]]\n",
      "obs= [[0.6301498  0.7764736  0.38163486]] reward= [-0.76167816] done= [False]\n",
      "Step 42 and action=[[-1.8773317]]\n",
      "obs= [[0.6032953 0.7975179 0.6823903]] reward= [-0.8084992] done= [False]\n",
      "Step 43 and action=[[-1.6078888]]\n",
      "obs= [[0.5610547  0.82777876 1.0393454 ]] reward= [-0.90139353] done= [False]\n",
      "Step 44 and action=[[-1.1373968]]\n",
      "obs= [[0.49790463 0.8672318  1.4895699 ]] reward= [-1.0602098] done= [False]\n",
      "Step 45 and action=[[-0.45604205]]\n",
      "obs= [[0.4055693 0.9140643 2.0715876]] reward= [-1.3237818] done= [False]\n",
      "Step 46 and action=[[0.73100686]]\n",
      "obs= [[0.27083674 0.96262527 2.866787  ]] reward= [-1.7595398] done= [False]\n",
      "Step 47 and action=[[1.6530943]]\n",
      "obs= [[0.08233284 0.99660486 3.83672   ]] reward= [-2.5055807] done= [False]\n",
      "Step 48 and action=[[1.8915029]]\n",
      "obs= [[-0.16027458  0.98707247  4.867899  ]] reward= [-3.6908655] done= [False]\n",
      "Step 49 and action=[[1.9592581]]\n",
      "obs= [[-0.4404263   0.89778876  5.902092  ]] reward= [-5.372493] done= [False]\n",
      "Step 50 and action=[[1.9704618]]\n",
      "obs= [[-0.7170938   0.69697666  6.8710027 ]] reward= [-7.595554] done= [False]\n",
      "Step 51 and action=[[1.9421859]]\n",
      "obs= [[-0.92607576  0.37733757  7.685063  ]] reward= [-10.34373] done= [False]\n",
      "Step 52 and action=[[1.9155631]]\n",
      "obs= [[-0.99991447 -0.01308007  8.        ]] reward= [-13.497913] done= [False]\n",
      "Step 53 and action=[[1.9401267]]\n",
      "obs= [[-0.91588855 -0.40143266  8.        ]] reward= [-16.191355] done= [False]\n",
      "Step 54 and action=[[1.9239302]]\n",
      "obs= [[-0.6877174  -0.72597855  7.987515  ]] reward= [-13.848479] done= [False]\n",
      "Step 55 and action=[[1.6993015]]\n",
      "obs= [[-0.36482385 -0.9310765   7.6979265 ]] reward= [-11.807804] done= [False]\n",
      "Step 56 and action=[[0.10535598]]\n",
      "obs= [[-0.0226707 -0.999743   7.0154223]] reward= [-9.705888] done= [False]\n",
      "Step 57 and action=[[-1.6343322]]\n",
      "obs= [[ 0.27477032 -0.9615099   6.0204654 ]] reward= [-7.4634295] done= [False]\n",
      "Step 58 and action=[[-1.9324815]]\n",
      "obs= [[ 0.5045182  -0.86340106  5.0094604 ]] reward= [-5.29875] done= [False]\n",
      "Step 59 and action=[[-1.9725515]]\n",
      "obs= [[ 0.66845185 -0.74375546  4.066027  ]] reward= [-3.599067] done= [False]\n",
      "Step 60 and action=[[-1.9811116]]\n",
      "obs= [[ 0.7787541 -0.6273293  3.2110438]] reward= [-2.3605516] done= [False]\n",
      "Step 61 and action=[[-1.9731219]]\n",
      "obs= [[ 0.84943116 -0.5276994   2.4445786 ]] reward= [-1.4948188] done= [False]\n",
      "Step 62 and action=[[-1.9142879]]\n",
      "obs= [[ 0.89255947 -0.45092967  1.7616608 ]] reward= [-0.910275] done= [False]\n",
      "Step 63 and action=[[-1.2331483]]\n",
      "obs= [[ 0.91875446 -0.39482942  1.2384913 ]] reward= [-0.53070897] done= [False]\n",
      "Step 64 and action=[[0.90671396]]\n",
      "obs= [[ 0.9386977  -0.34474146  1.0783763 ]] reward= [-0.3189485] done= [False]\n",
      "Step 65 and action=[[0.6900079]]\n",
      "obs= [[ 0.9536072  -0.30105367  0.92332137]] reward= [-0.24064381] done= [False]\n",
      "Step 66 and action=[[0.6203046]]\n",
      "obs= [[ 0.96475947 -0.26313332  0.7905768 ]] reward= [-0.17914933] done= [False]\n",
      "Step 67 and action=[[0.5939455]]\n",
      "obs= [[ 0.9731734  -0.23007306  0.6823186 ]] reward= [-0.1337531] done= [False]\n",
      "Step 68 and action=[[0.53255177]]\n",
      "obs= [[ 0.97953254 -0.20128578  0.5896466 ]] reward= [-0.10073458] done= [False]\n",
      "Step 69 and action=[[0.44256997]]\n",
      "obs= [[ 0.9843028  -0.1764878   0.50506777]] reward= [-0.07603944] done= [False]\n",
      "Step 70 and action=[[0.358083]]\n",
      "obs= [[ 0.98784167 -0.15546314  0.42641437]] reward= [-0.05711438] done= [False]\n",
      "Step 71 and action=[[0.2978716]]\n",
      "obs= [[ 0.990442   -0.13793015  0.35449776]] reward= [-0.04263783] done= [False]\n",
      "Step 72 and action=[[0.27362704]]\n",
      "obs= [[ 0.9923507  -0.12345099  0.2920942 ]] reward= [-0.03178847] done= [False]\n",
      "Step 73 and action=[[0.27380037]]\n",
      "obs= [[ 0.9937638  -0.11150566  0.24057601]] reward= [-0.02392517] done= [False]\n",
      "Step 74 and action=[[0.27455425]]\n",
      "obs= [[ 0.99481964 -0.10165584  0.19812989]] reward= [-0.01834854] done= [False]\n",
      "Step 75 and action=[[0.27452755]]\n",
      "obs= [[ 0.9956154  -0.09354135  0.16306715]] reward= [-0.0143706] done= [False]\n",
      "Step 76 and action=[[0.26433945]]\n",
      "obs= [[ 0.9962135  -0.08694037  0.13256206]] reward= [-0.01150467] done= [False]\n",
      "Step 77 and action=[[0.25385928]]\n",
      "obs= [[ 0.996658   -0.08168723  0.10543567]] reward= [-0.00939954] done= [False]\n",
      "Step 78 and action=[[0.25608516]]\n",
      "obs= [[ 0.9969868  -0.07757141  0.08258302]] reward= [-0.00786501] done= [False]\n",
      "Step 79 and action=[[0.26065588]]\n",
      "obs= [[ 0.9972281  -0.07440533  0.06350283]] reward= [-0.00677936] done= [False]\n",
      "Step 80 and action=[[0.27246618]]\n",
      "obs= [[ 0.9974058  -0.07198332  0.04856876]] reward= [-0.00602396] done= [False]\n",
      "Step 81 and action=[[0.28356028]]\n",
      "obs= [[ 0.99753773 -0.07013217  0.03711531]] reward= [-0.00550686] done= [False]\n",
      "Step 82 and action=[[0.2923534]]\n",
      "obs= [[ 0.9976362  -0.068717    0.02836919]] reward= [-0.00514982] done= [False]\n",
      "Step 83 and action=[[0.2990172]]\n",
      "obs= [[ 0.9977101  -0.06763519  0.02168402]] reward= [-0.00489943] done= [False]\n",
      "Step 84 and action=[[0.30400872]]\n",
      "obs= [[ 0.9977658  -0.06680927  0.01655894]] reward= [-0.00472101] done= [False]\n",
      "Step 85 and action=[[0.30796075]]\n",
      "obs= [[ 0.9978078  -0.06617839  0.0126461 ]] reward= [-0.00459245] done= [False]\n",
      "Step 86 and action=[[0.31096673]]\n",
      "obs= [[ 0.9978396  -0.0656964   0.00965731]] reward= [-0.00449873] done= [False]\n",
      "Step 87 and action=[[0.31325912]]\n",
      "obs= [[ 0.9978638  -0.0653286   0.00737388]] reward= [-0.00442969] done= [False]\n",
      "Step 88 and action=[[0.31502485]]\n",
      "obs= [[ 0.9978821  -0.06504786  0.00563116]] reward= [-0.00437864] done= [False]\n",
      "Step 89 and action=[[0.31637192]]\n",
      "obs= [[ 0.99789613 -0.06483326  0.00430105]] reward= [-0.00434052] done= [False]\n",
      "Step 90 and action=[[0.31738806]]\n",
      "obs= [[ 0.99790674 -0.06466958  0.00328432]] reward= [-0.00431183] done= [False]\n",
      "Step 91 and action=[[0.3181777]]\n",
      "obs= [[ 0.99791485 -0.06454443  0.00250879]] reward= [-0.0042903] done= [False]\n",
      "Step 92 and action=[[0.31877017]]\n",
      "obs= [[ 0.997921   -0.06444878  0.00191599]] reward= [-0.00427408] done= [False]\n",
      "Step 93 and action=[[0.31922293]]\n",
      "obs= [[ 0.9979257  -0.06437598  0.00146284]] reward= [-0.00426167] done= [False]\n",
      "Step 94 and action=[[0.31958055]]\n",
      "obs= [[ 0.99792933 -0.0643203   0.00111794]] reward= [-0.0042524] done= [False]\n",
      "Step 95 and action=[[0.31984305]]\n",
      "obs= [[ 9.979321e-01 -6.427748e-02  8.541716e-04]] reward= [-0.00424524] done= [False]\n",
      "Step 96 and action=[[0.32003498]]\n",
      "obs= [[ 9.9793416e-01 -6.4245120e-02  6.5131014e-04]] reward= [-0.00423978] done= [False]\n",
      "Step 97 and action=[[0.3202033]]\n",
      "obs= [[ 9.9793571e-01 -6.4220376e-02  4.9796526e-04]] reward= [-0.00423569] done= [False]\n",
      "Step 98 and action=[[0.32032013]]\n",
      "obs= [[ 9.9793696e-01 -6.4201340e-02  3.8070205e-04]] reward= [-0.00423256] done= [False]\n",
      "Step 99 and action=[[0.3204062]]\n",
      "obs= [[ 9.9793786e-01 -6.4187065e-02  2.9062750e-04]] reward= [-0.00423015] done= [False]\n",
      "Step 100 and action=[[0.32048416]]\n",
      "obs= [[ 9.979386e-01 -6.417612e-02  2.229524e-04]] reward= [-0.00422836] done= [False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85715/3941152309.py:150: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return self._get_obs(), -float(costs), False, False, {}\n",
      "/home/egtechlab/anaconda3/envs/mujoco_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "# using the vecenv\n",
    "obs = vec_env.reset()\n",
    "n_steps = 100\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(f\"Step {step + 1} and action={action}\")\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    vec_env.render() # ........x..: x is current position\n",
    "    if done:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
