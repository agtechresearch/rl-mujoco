{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stable baseline3 에 훈련 결과를 차트로 뽑아주는 기능이 있는지 살펴보고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 0, 1, 2, 2, 4, 3, 4, 4, 1, 2, 2, 1, 0, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "size = 4\n",
    "spaces.MultiDiscrete([size+1] * (size**2)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "\n",
      "[0 0 0 4]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "\n",
      "[0 0 0 4]\n",
      "[0 0 0 1]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "\n",
      "[0 0 0 4]\n",
      "[0 0 0 1]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "\n",
      "[0 0 0 4]\n",
      "[0 0 0 1]\n",
      "[0 1 0 0]\n",
      "[0 0 0 0]\n",
      "\n",
      "[0 0 0 4]\n",
      "[0 0 0 1]\n",
      "[0 1 0 0]\n",
      "[0 0 0 0]\n",
      "\n",
      "[0 0 0 4]\n",
      "[0 0 0 1]\n",
      "[0 1 3 0]\n",
      "[0 0 0 0]\n",
      "\n",
      "[0 0 0 4]\n",
      "[0 0 0 1]\n",
      "[0 1 3 0]\n",
      "[0 0 0 0]\n",
      "\n",
      "[0 0 0 4]\n",
      "[0 0 0 1]\n",
      "[0 1 3 0]\n",
      "[0 0 0 0]\n",
      "\n",
      "[0 0 0 4]\n",
      "[0 0 0 1]\n",
      "[0 1 3 0]\n",
      "[0 0 0 0]\n",
      "\n",
      "[0 3 0 4]\n",
      "[0 0 0 1]\n",
      "[0 1 3 0]\n",
      "[0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SudokuEnv(gym.Env):\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.half_size = size // 2\n",
    "        self.action_space = spaces.Discrete(size**3)\n",
    "        self.observation_space = spaces.MultiDiscrete([size+1] * (size**2))  # 4x4 Sudoku 게임판을 1차원 배열로 표현\n",
    "        self.actions = np.array([\n",
    "            [[(x, y, v) for v in range(1, size+1)] for y in range(size)] for x in range(size)\n",
    "        ]).reshape(-1, 3)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.board = np.zeros(self.size**2, dtype=np.int32)\n",
    "        return self.board, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y, num = self.actions[action]\n",
    "\n",
    "        # 해당 위치에 숫자를 채우고 유효성을 검사\n",
    "        reward = 0\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        # 유효성 검사\n",
    "        if self.is_valid_move(x, y, num):\n",
    "            self.board[x * 4 + y] = num\n",
    "            if self.is_game_over():\n",
    "                done = True\n",
    "                reward = 1  # 승리 보상\n",
    "        else:\n",
    "            reward = -10  # 잘못된 행동에 대한 패널티\n",
    "\n",
    "        return self.board, reward, done, done, info\n",
    "\n",
    "    def is_valid_move(self, x, y, num):\n",
    "        # 행, 열, 2x2 블록에 중복된 숫자가 있는지 확인\n",
    "        if self.board[x * self.size + y] != 0:\n",
    "            return False\n",
    "        \n",
    "        row_start = (x // self.half_size) * self.half_size\n",
    "        col_start = (y // self.half_size) * self.half_size\n",
    "        temp = self.board.reshape(self.size, self.size)\n",
    "\n",
    "        # print(num, row_start, col_start)\n",
    "        # print(self.board[x*self.size : (x+1)*self.size])\n",
    "        # print(self.board[y::self.size])\n",
    "        # print(temp)\n",
    "        # print(temp[row_start : row_start+self.half_size, col_start : col_start+self.half_size])\n",
    "        # print()\n",
    "\n",
    "        if (num in self.board[x*self.size : (x+1)*self.size]) or \\\n",
    "           (num in self.board[y::self.size]):\n",
    "            return False\n",
    "        if  (num in temp[row_start : row_start+self.half_size,\n",
    "                col_start : col_start+self.half_size]):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def is_game_over(self):\n",
    "        # 게임이 종료되었는지 확인\n",
    "        return 0 not in self.board\n",
    "\n",
    "    def render(self):\n",
    "        # 현재 게임판 상태 출력\n",
    "        # print(np.array(self.board).reshape(self.size, self.size))\n",
    "        for i in range(self.size):\n",
    "            print(self.board[i*self.size:i*self.size+self.size])\n",
    "        print()\n",
    "\n",
    "# 환경 테스트\n",
    "env = SudokuEnv()\n",
    "observation = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # 무작위 행동 선택\n",
    "    observation, reward, done, _, _ = env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = SudokuEnv()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1840 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1505         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055992883 |\n",
      "|    clip_fraction        | 0.0197       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.16        |\n",
      "|    explained_variance   | 0.00307      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.19e+04     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00976     |\n",
      "|    value_loss           | 2.53e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1422        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004021289 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.15       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.26e+04    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    value_loss           | 2.62e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1383         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052564167 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.15        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.25e+04     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00739     |\n",
      "|    value_loss           | 2.58e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1363        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005377752 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.15       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.22e+04    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00696    |\n",
      "|    value_loss           | 2.55e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1347         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044408026 |\n",
      "|    clip_fraction        | 0.0122       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.14        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.22e+04     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00616     |\n",
      "|    value_loss           | 2.52e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1343         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061657606 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.13        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+04      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    value_loss           | 2.48e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1336         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056264107 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.12        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18e+04     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0068      |\n",
      "|    value_loss           | 2.45e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1335         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061721187 |\n",
      "|    clip_fraction        | 0.0108       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.11        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18e+04     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00721     |\n",
      "|    value_loss           | 2.42e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1331        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006602119 |\n",
      "|    clip_fraction        | 0.0238      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.1        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.14e+04    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    value_loss           | 2.39e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1327         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057014385 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.09        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.13e+04     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00841     |\n",
      "|    value_loss           | 2.36e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1325         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070466697 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.08        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.11e+04     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0087      |\n",
      "|    value_loss           | 2.32e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1323         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060595237 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.08        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.09e+04     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00619     |\n",
      "|    value_loss           | 2.29e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1320         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072116973 |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.07        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.09e+04     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00579     |\n",
      "|    value_loss           | 2.26e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1318        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008980602 |\n",
      "|    clip_fraction        | 0.0388      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.04       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.08e+04    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00864    |\n",
      "|    value_loss           | 2.23e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1318         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072771627 |\n",
      "|    clip_fraction        | 0.0452       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.03        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+04     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00944     |\n",
      "|    value_loss           | 2.2e+04      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1317        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007397688 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.05e+04    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0092     |\n",
      "|    value_loss           | 2.17e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1316        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009212866 |\n",
      "|    clip_fraction        | 0.0559      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4          |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.04e+04    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 2.14e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1317        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008304704 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.01       |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.02e+04    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00924    |\n",
      "|    value_loss           | 2.11e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1316        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008654172 |\n",
      "|    clip_fraction        | 0.0648      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4          |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.9e+03     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    value_loss           | 2.08e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1317         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083859805 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.99        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.9e+03      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00986     |\n",
      "|    value_loss           | 2.05e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1316        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009443941 |\n",
      "|    clip_fraction        | 0.0319      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.99       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.66e+03    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00836    |\n",
      "|    value_loss           | 2.02e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1315        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009022372 |\n",
      "|    clip_fraction        | 0.0498      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.96       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.58e+03    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00938    |\n",
      "|    value_loss           | 1.99e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1313        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008248792 |\n",
      "|    clip_fraction        | 0.0316      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.92       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.33e+03    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00737    |\n",
      "|    value_loss           | 1.96e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1309        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006789652 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.92       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.27e+03    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0082     |\n",
      "|    value_loss           | 1.93e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1308        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011138307 |\n",
      "|    clip_fraction        | 0.0745      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.92       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.04e+03    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00928    |\n",
      "|    value_loss           | 1.9e+04     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1308         |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100310305 |\n",
      "|    clip_fraction        | 0.0748       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.93        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.99e+03     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00941     |\n",
      "|    value_loss           | 1.87e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1307         |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076401476 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.93        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.61e+03     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00717     |\n",
      "|    value_loss           | 1.84e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1306        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010502376 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.92       |\n",
      "|    explained_variance   | 2.38e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.71e+03    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00994    |\n",
      "|    value_loss           | 1.81e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1307        |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009384269 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.9        |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.62e+03    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 1.79e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1308       |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 48         |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00915379 |\n",
      "|    clip_fraction        | 0.0479     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.89      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.33e+03   |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.00854   |\n",
      "|    value_loss           | 1.76e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1307        |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010694837 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.89       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.06e+03    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 1.73e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1306        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008577947 |\n",
      "|    clip_fraction        | 0.0519      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.86       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.11e+03    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00894    |\n",
      "|    value_loss           | 1.7e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1304        |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008326339 |\n",
      "|    clip_fraction        | 0.0517      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.87       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.08e+03    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00826    |\n",
      "|    value_loss           | 1.68e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1300        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009736669 |\n",
      "|    clip_fraction        | 0.0716      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.86       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.89e+03    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00944    |\n",
      "|    value_loss           | 1.65e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1293       |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 56         |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01174913 |\n",
      "|    clip_fraction        | 0.095      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.83      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.76e+03   |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    value_loss           | 1.62e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1293        |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012654286 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.85       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.63e+03    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 1.6e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1293        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010395077 |\n",
      "|    clip_fraction        | 0.0896      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.84       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.51e+03    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00978    |\n",
      "|    value_loss           | 1.57e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1292        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009157302 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.84       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.4e+03     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00693    |\n",
      "|    value_loss           | 1.54e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1290         |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081494935 |\n",
      "|    clip_fraction        | 0.0512       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.8         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.24e+03     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00813     |\n",
      "|    value_loss           | 1.52e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1290        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012018636 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.75       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.05e+03    |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00948    |\n",
      "|    value_loss           | 1.49e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1290        |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010765612 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.72       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.98e+03    |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 1.47e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1290        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010762256 |\n",
      "|    clip_fraction        | 0.0569      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.69       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.89e+03    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.008      |\n",
      "|    value_loss           | 1.44e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1290        |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013558043 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.62       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.74e+03    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 1.42e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1291        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009372339 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.59       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.65e+03    |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    value_loss           | 1.39e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1290        |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009471476 |\n",
      "|    clip_fraction        | 0.0892      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.57       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.54e+03    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00865    |\n",
      "|    value_loss           | 1.37e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1290         |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087779295 |\n",
      "|    clip_fraction        | 0.0586       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.57        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.38e+03     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00695     |\n",
      "|    value_loss           | 1.34e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1290        |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010204432 |\n",
      "|    clip_fraction        | 0.0746      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.59       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.26e+03    |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00933    |\n",
      "|    value_loss           | 1.32e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1291       |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 77         |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01345904 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.52      |\n",
      "|    explained_variance   | 1.79e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.21e+03   |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    value_loss           | 1.3e+04    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1291        |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011427367 |\n",
      "|    clip_fraction        | 0.0799      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.47       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.02e+03    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00786    |\n",
      "|    value_loss           | 1.27e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1290        |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014369884 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.39       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.91e+03    |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00914    |\n",
      "|    value_loss           | 1.25e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1291        |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015789853 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.24       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.79e+03    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 1.23e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1291        |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015856555 |\n",
      "|    clip_fraction        | 0.0765      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.16       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.74e+03    |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0078     |\n",
      "|    value_loss           | 1.2e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1292        |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009468921 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.11       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.47e+03    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00898    |\n",
      "|    value_loss           | 1.18e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1292       |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 87         |\n",
      "|    total_timesteps      | 112640     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01065927 |\n",
      "|    clip_fraction        | 0.0582     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3         |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.53e+03   |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.00792   |\n",
      "|    value_loss           | 1.16e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1292       |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 88         |\n",
      "|    total_timesteps      | 114688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01017298 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.99      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.3e+03    |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    value_loss           | 1.14e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1292       |\n",
      "|    iterations           | 57         |\n",
      "|    time_elapsed         | 90         |\n",
      "|    total_timesteps      | 116736     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01420222 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.1       |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.3e+03    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.00885   |\n",
      "|    value_loss           | 1.11e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1292        |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008445245 |\n",
      "|    clip_fraction        | 0.0635      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.08       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.16e+03    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00504    |\n",
      "|    value_loss           | 1.09e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1292        |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013342392 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.02       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.95e+03    |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0087     |\n",
      "|    value_loss           | 1.07e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1292        |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008981245 |\n",
      "|    clip_fraction        | 0.0694      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.99       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.96e+03    |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00693    |\n",
      "|    value_loss           | 1.05e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1292        |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017031156 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.85e+03    |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00737    |\n",
      "|    value_loss           | 1.03e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1292        |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018592175 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.56       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.68e+03    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00924    |\n",
      "|    value_loss           | 1.01e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1292        |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007550378 |\n",
      "|    clip_fraction        | 0.0686      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.65e+03    |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00605    |\n",
      "|    value_loss           | 9.85e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1292        |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016504202 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.35       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.58e+03    |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00931    |\n",
      "|    value_loss           | 9.65e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1293         |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 133120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068082316 |\n",
      "|    clip_fraction        | 0.0804       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.26        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.43e+03     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00701     |\n",
      "|    value_loss           | 9.44e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1292        |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007641078 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.18       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.33e+03    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00457    |\n",
      "|    value_loss           | 9.24e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1293        |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008853879 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.2e+03     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00704    |\n",
      "|    value_loss           | 9.04e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1293        |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008979481 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.12e+03    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0036     |\n",
      "|    value_loss           | 8.85e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1293         |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 109          |\n",
      "|    total_timesteps      | 141312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059371255 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.84        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.02e+03     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    value_loss           | 8.65e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1293         |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 110          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055357865 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.79        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.85e+03     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    value_loss           | 8.46e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1293         |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 112          |\n",
      "|    total_timesteps      | 145408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053790705 |\n",
      "|    clip_fraction        | 0.0643       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.76        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.78e+03     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00344     |\n",
      "|    value_loss           | 8.27e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1293         |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 113          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067102555 |\n",
      "|    clip_fraction        | 0.0481       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.59        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.81e+03     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00426     |\n",
      "|    value_loss           | 8.08e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1294        |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004992797 |\n",
      "|    clip_fraction        | 0.0552      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.66e+03    |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00543    |\n",
      "|    value_loss           | 7.89e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1294         |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036136655 |\n",
      "|    clip_fraction        | 0.0504       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.52        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.57e+03     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00419     |\n",
      "|    value_loss           | 7.71e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1294        |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005680034 |\n",
      "|    clip_fraction        | 0.0598      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.47e+03    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    value_loss           | 7.53e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1294         |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 120          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037265955 |\n",
      "|    clip_fraction        | 0.0493       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.33e+03     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    value_loss           | 7.35e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1294        |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005922894 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.36e+03    |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | 0.000935    |\n",
      "|    value_loss           | 7.17e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1294         |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 123          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048965276 |\n",
      "|    clip_fraction        | 0.0581       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.24e+03     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00466     |\n",
      "|    value_loss           | 6.99e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1294         |\n",
      "|    iterations           | 79           |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 161792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036829608 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.16e+03     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00436     |\n",
      "|    value_loss           | 6.82e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044618445 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.09e+03     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    value_loss           | 6.65e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1294        |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007440441 |\n",
      "|    clip_fraction        | 0.0897      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.92e+03    |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00406    |\n",
      "|    value_loss           | 6.48e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1294        |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005807461 |\n",
      "|    clip_fraction        | 0.0945      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.95e+03    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0041     |\n",
      "|    value_loss           | 6.31e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1294         |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 131          |\n",
      "|    total_timesteps      | 169984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072545856 |\n",
      "|    clip_fraction        | 0.0751       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.81e+03     |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    value_loss           | 6.15e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 84           |\n",
      "|    time_elapsed         | 132          |\n",
      "|    total_timesteps      | 172032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053691217 |\n",
      "|    clip_fraction        | 0.0705       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.883       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.78e+03     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00388     |\n",
      "|    value_loss           | 5.99e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1295        |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004278356 |\n",
      "|    clip_fraction        | 0.0612      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.882      |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.69e+03    |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    value_loss           | 5.83e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1294         |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 176128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031267367 |\n",
      "|    clip_fraction        | 0.0464       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.897       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.59e+03     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.000162    |\n",
      "|    value_loss           | 5.67e+03     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1294      |\n",
      "|    iterations           | 87        |\n",
      "|    time_elapsed         | 137       |\n",
      "|    total_timesteps      | 178176    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0043285 |\n",
      "|    clip_fraction        | 0.0572    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.796    |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.48e+03  |\n",
      "|    n_updates            | 860       |\n",
      "|    policy_gradient_loss | -0.00218  |\n",
      "|    value_loss           | 5.51e+03  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1294         |\n",
      "|    iterations           | 88           |\n",
      "|    time_elapsed         | 139          |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031622015 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.801       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.47e+03     |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 5.36e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1294         |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 140          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024885796 |\n",
      "|    clip_fraction        | 0.0082       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.836       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.37e+03     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    value_loss           | 5.21e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1294         |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 142          |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059902146 |\n",
      "|    clip_fraction        | 0.0779       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.769       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.33e+03     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.00281     |\n",
      "|    value_loss           | 5.06e+03     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1294       |\n",
      "|    iterations           | 91         |\n",
      "|    time_elapsed         | 143        |\n",
      "|    total_timesteps      | 186368     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00360601 |\n",
      "|    clip_fraction        | 0.0428     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.788     |\n",
      "|    explained_variance   | 2.98e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.24e+03   |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.00145   |\n",
      "|    value_loss           | 4.91e+03   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1294         |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 145          |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010434628 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.847       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.19e+03     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.000781    |\n",
      "|    value_loss           | 4.76e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 147          |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039386163 |\n",
      "|    clip_fraction        | 0.0144       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.776       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.13e+03     |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.000929    |\n",
      "|    value_loss           | 4.62e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1295        |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005331983 |\n",
      "|    clip_fraction        | 0.0698      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.65       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.06e+03    |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 4.48e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1295        |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003368617 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.98e+03    |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 4.34e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 96           |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019740365 |\n",
      "|    clip_fraction        | 0.00986      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.53        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.91e+03     |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.000767    |\n",
      "|    value_loss           | 4.21e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 153          |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024451544 |\n",
      "|    clip_fraction        | 0.0451       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.555       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.83e+03     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 4.07e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 154          |\n",
      "|    total_timesteps      | 200704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033870644 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.526       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.8e+03      |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.000342    |\n",
      "|    value_loss           | 3.94e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 156          |\n",
      "|    total_timesteps      | 202752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025992543 |\n",
      "|    clip_fraction        | 0.0521       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.513       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.74e+03     |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.00294     |\n",
      "|    value_loss           | 3.81e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 158          |\n",
      "|    total_timesteps      | 204800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030849685 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.44        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.67e+03     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 3.68e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1295        |\n",
      "|    iterations           | 101         |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 206848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002570213 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.6e+03     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.00068    |\n",
      "|    value_loss           | 3.56e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1295        |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 208896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001901711 |\n",
      "|    clip_fraction        | 0.0112      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.33       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.55e+03    |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.00063    |\n",
      "|    value_loss           | 3.43e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 103          |\n",
      "|    time_elapsed         | 162          |\n",
      "|    total_timesteps      | 210944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027216838 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.266       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.47e+03     |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    value_loss           | 3.31e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 164          |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010473258 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.235       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.44e+03     |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.000436    |\n",
      "|    value_loss           | 3.19e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1295          |\n",
      "|    iterations           | 105           |\n",
      "|    time_elapsed         | 165           |\n",
      "|    total_timesteps      | 215040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00067558244 |\n",
      "|    clip_fraction        | 0.00854       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.206        |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.38e+03      |\n",
      "|    n_updates            | 1040          |\n",
      "|    policy_gradient_loss | -0.000818     |\n",
      "|    value_loss           | 3.08e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1295          |\n",
      "|    iterations           | 106           |\n",
      "|    time_elapsed         | 167           |\n",
      "|    total_timesteps      | 217088        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054846273 |\n",
      "|    clip_fraction        | 0.00166       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.211        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.32e+03      |\n",
      "|    n_updates            | 1050          |\n",
      "|    policy_gradient_loss | -0.000433     |\n",
      "|    value_loss           | 2.96e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 169          |\n",
      "|    total_timesteps      | 219136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008676045 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.231       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+03     |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0005      |\n",
      "|    value_loss           | 2.85e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1295          |\n",
      "|    iterations           | 108           |\n",
      "|    time_elapsed         | 170           |\n",
      "|    total_timesteps      | 221184        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021948144 |\n",
      "|    clip_fraction        | 0.0156        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.249        |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.22e+03      |\n",
      "|    n_updates            | 1070          |\n",
      "|    policy_gradient_loss | -0.000456     |\n",
      "|    value_loss           | 2.74e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1295          |\n",
      "|    iterations           | 109           |\n",
      "|    time_elapsed         | 172           |\n",
      "|    total_timesteps      | 223232        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044102693 |\n",
      "|    clip_fraction        | 0.00322       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.23         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+03      |\n",
      "|    n_updates            | 1080          |\n",
      "|    policy_gradient_loss | -1.17e-05     |\n",
      "|    value_loss           | 2.63e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1296         |\n",
      "|    iterations           | 110          |\n",
      "|    time_elapsed         | 173          |\n",
      "|    total_timesteps      | 225280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011281063 |\n",
      "|    clip_fraction        | 0.0262       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.19        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.08e+03     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.000916    |\n",
      "|    value_loss           | 2.52e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1296          |\n",
      "|    iterations           | 111           |\n",
      "|    time_elapsed         | 175           |\n",
      "|    total_timesteps      | 227328        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039340687 |\n",
      "|    clip_fraction        | 0.0112        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.173        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.07e+03      |\n",
      "|    n_updates            | 1100          |\n",
      "|    policy_gradient_loss | -0.000499     |\n",
      "|    value_loss           | 2.42e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1296          |\n",
      "|    iterations           | 112           |\n",
      "|    time_elapsed         | 176           |\n",
      "|    total_timesteps      | 229376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00058910757 |\n",
      "|    clip_fraction        | 0.0112        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.198        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.03e+03      |\n",
      "|    n_updates            | 1110          |\n",
      "|    policy_gradient_loss | -0.0006       |\n",
      "|    value_loss           | 2.32e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1296         |\n",
      "|    iterations           | 113          |\n",
      "|    time_elapsed         | 178          |\n",
      "|    total_timesteps      | 231424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017414634 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 971          |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | 0.0012       |\n",
      "|    value_loss           | 2.22e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1296         |\n",
      "|    iterations           | 114          |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 233472       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009101613 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 917          |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | 4.6e-05      |\n",
      "|    value_loss           | 2.12e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1296         |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 181          |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014808695 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.145       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 844          |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 2.03e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1297          |\n",
      "|    iterations           | 116           |\n",
      "|    time_elapsed         | 183           |\n",
      "|    total_timesteps      | 237568        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032935216 |\n",
      "|    clip_fraction        | 0.0021        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.133        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 829           |\n",
      "|    n_updates            | 1150          |\n",
      "|    policy_gradient_loss | -0.000136     |\n",
      "|    value_loss           | 1.93e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1297          |\n",
      "|    iterations           | 117           |\n",
      "|    time_elapsed         | 184           |\n",
      "|    total_timesteps      | 239616        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00050528103 |\n",
      "|    clip_fraction        | 0.00571       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.114        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 784           |\n",
      "|    n_updates            | 1160          |\n",
      "|    policy_gradient_loss | -0.000556     |\n",
      "|    value_loss           | 1.84e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1297          |\n",
      "|    iterations           | 118           |\n",
      "|    time_elapsed         | 186           |\n",
      "|    total_timesteps      | 241664        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4063258e-05 |\n",
      "|    clip_fraction        | 0.00483       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0949       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 756           |\n",
      "|    n_updates            | 1170          |\n",
      "|    policy_gradient_loss | -2.75e-05     |\n",
      "|    value_loss           | 1.76e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1297         |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 187          |\n",
      "|    total_timesteps      | 243712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010375789 |\n",
      "|    clip_fraction        | 0.013        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.121       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 712          |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | 0.000498     |\n",
      "|    value_loss           | 1.67e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1297          |\n",
      "|    iterations           | 120           |\n",
      "|    time_elapsed         | 189           |\n",
      "|    total_timesteps      | 245760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035466137 |\n",
      "|    clip_fraction        | 0.00801       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.131        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 672           |\n",
      "|    n_updates            | 1190          |\n",
      "|    policy_gradient_loss | -0.000273     |\n",
      "|    value_loss           | 1.58e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1297         |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 190          |\n",
      "|    total_timesteps      | 247808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006394733 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.102       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 624          |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.000561    |\n",
      "|    value_loss           | 1.5e+03      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1297          |\n",
      "|    iterations           | 122           |\n",
      "|    time_elapsed         | 192           |\n",
      "|    total_timesteps      | 249856        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043989398 |\n",
      "|    clip_fraction        | 0.0116        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.112        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 600           |\n",
      "|    n_updates            | 1210          |\n",
      "|    policy_gradient_loss | -0.000498     |\n",
      "|    value_loss           | 1.42e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1297          |\n",
      "|    iterations           | 123           |\n",
      "|    time_elapsed         | 194           |\n",
      "|    total_timesteps      | 251904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023039235 |\n",
      "|    clip_fraction        | 0.00156       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.111        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 568           |\n",
      "|    n_updates            | 1220          |\n",
      "|    policy_gradient_loss | -1.92e-06     |\n",
      "|    value_loss           | 1.35e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1297          |\n",
      "|    iterations           | 124           |\n",
      "|    time_elapsed         | 195           |\n",
      "|    total_timesteps      | 253952        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037231695 |\n",
      "|    clip_fraction        | 0.0136        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.128        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 528           |\n",
      "|    n_updates            | 1230          |\n",
      "|    policy_gradient_loss | -0.000426     |\n",
      "|    value_loss           | 1.27e+03      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1297        |\n",
      "|    iterations           | 125         |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 256000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.30779e-05 |\n",
      "|    clip_fraction        | 0.0063      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.111      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 490         |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -7.85e-05   |\n",
      "|    value_loss           | 1.2e+03     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1297         |\n",
      "|    iterations           | 126          |\n",
      "|    time_elapsed         | 198          |\n",
      "|    total_timesteps      | 258048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005528639 |\n",
      "|    clip_fraction        | 0.00566      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.107       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 468          |\n",
      "|    n_updates            | 1250         |\n",
      "|    policy_gradient_loss | 4.3e-05      |\n",
      "|    value_loss           | 1.13e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1297          |\n",
      "|    iterations           | 127           |\n",
      "|    time_elapsed         | 200           |\n",
      "|    total_timesteps      | 260096        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038009256 |\n",
      "|    clip_fraction        | 0.00522       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0866       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 436           |\n",
      "|    n_updates            | 1260          |\n",
      "|    policy_gradient_loss | -0.000192     |\n",
      "|    value_loss           | 1.06e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 128           |\n",
      "|    time_elapsed         | 201           |\n",
      "|    total_timesteps      | 262144        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025528696 |\n",
      "|    clip_fraction        | 0.00542       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0702       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 404           |\n",
      "|    n_updates            | 1270          |\n",
      "|    policy_gradient_loss | -0.00119      |\n",
      "|    value_loss           | 990           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 129           |\n",
      "|    time_elapsed         | 203           |\n",
      "|    total_timesteps      | 264192        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038105244 |\n",
      "|    clip_fraction        | 0.00288       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0667       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 375           |\n",
      "|    n_updates            | 1280          |\n",
      "|    policy_gradient_loss | 0.00202       |\n",
      "|    value_loss           | 926           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 130           |\n",
      "|    time_elapsed         | 205           |\n",
      "|    total_timesteps      | 266240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010949673 |\n",
      "|    clip_fraction        | 0.0041        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0939       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 329           |\n",
      "|    n_updates            | 1290          |\n",
      "|    policy_gradient_loss | -0.000602     |\n",
      "|    value_loss           | 864           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1298         |\n",
      "|    iterations           | 131          |\n",
      "|    time_elapsed         | 206          |\n",
      "|    total_timesteps      | 268288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002818167 |\n",
      "|    clip_fraction        | 0.00366      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0832      |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 310          |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -4.12e-05    |\n",
      "|    value_loss           | 803          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1298         |\n",
      "|    iterations           | 132          |\n",
      "|    time_elapsed         | 208          |\n",
      "|    total_timesteps      | 270336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004566875 |\n",
      "|    clip_fraction        | 0.00923      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0652      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 290          |\n",
      "|    n_updates            | 1310         |\n",
      "|    policy_gradient_loss | -0.000398    |\n",
      "|    value_loss           | 745          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 133           |\n",
      "|    time_elapsed         | 209           |\n",
      "|    total_timesteps      | 272384        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025314937 |\n",
      "|    clip_fraction        | 0.00615       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0742       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 265           |\n",
      "|    n_updates            | 1320          |\n",
      "|    policy_gradient_loss | -0.000578     |\n",
      "|    value_loss           | 690           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 134           |\n",
      "|    time_elapsed         | 211           |\n",
      "|    total_timesteps      | 274432        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027221453 |\n",
      "|    clip_fraction        | 0.0061        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0594       |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 243           |\n",
      "|    n_updates            | 1330          |\n",
      "|    policy_gradient_loss | -0.000642     |\n",
      "|    value_loss           | 636           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1298         |\n",
      "|    iterations           | 135          |\n",
      "|    time_elapsed         | 212          |\n",
      "|    total_timesteps      | 276480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004994514 |\n",
      "|    clip_fraction        | 0.00742      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0689      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 221          |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | 0.00365      |\n",
      "|    value_loss           | 585          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 136           |\n",
      "|    time_elapsed         | 214           |\n",
      "|    total_timesteps      | 278528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034732526 |\n",
      "|    clip_fraction        | 0.0063        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0567       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 201           |\n",
      "|    n_updates            | 1350          |\n",
      "|    policy_gradient_loss | -0.000574     |\n",
      "|    value_loss           | 535           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1298         |\n",
      "|    iterations           | 137          |\n",
      "|    time_elapsed         | 216          |\n",
      "|    total_timesteps      | 280576       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.964943e-05 |\n",
      "|    clip_fraction        | 0.00376      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0641      |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 180          |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.000444    |\n",
      "|    value_loss           | 488          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1298         |\n",
      "|    iterations           | 138          |\n",
      "|    time_elapsed         | 217          |\n",
      "|    total_timesteps      | 282624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002775363 |\n",
      "|    clip_fraction        | 0.00679      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0603      |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 160          |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | -0.000174    |\n",
      "|    value_loss           | 443          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 139           |\n",
      "|    time_elapsed         | 219           |\n",
      "|    total_timesteps      | 284672        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025069073 |\n",
      "|    clip_fraction        | 0.00376       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0746       |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 141           |\n",
      "|    n_updates            | 1380          |\n",
      "|    policy_gradient_loss | 3.32e-05      |\n",
      "|    value_loss           | 401           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 140           |\n",
      "|    time_elapsed         | 220           |\n",
      "|    total_timesteps      | 286720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037094072 |\n",
      "|    clip_fraction        | 0.00684       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.07         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 125           |\n",
      "|    n_updates            | 1390          |\n",
      "|    policy_gradient_loss | 9.6e-05       |\n",
      "|    value_loss           | 360           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 141           |\n",
      "|    time_elapsed         | 222           |\n",
      "|    total_timesteps      | 288768        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034864375 |\n",
      "|    clip_fraction        | 0.00937       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0861       |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 109           |\n",
      "|    n_updates            | 1400          |\n",
      "|    policy_gradient_loss | 0.00401       |\n",
      "|    value_loss           | 322           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1298         |\n",
      "|    iterations           | 142          |\n",
      "|    time_elapsed         | 223          |\n",
      "|    total_timesteps      | 290816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004578986 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0693      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 95.2         |\n",
      "|    n_updates            | 1410         |\n",
      "|    policy_gradient_loss | -0.000315    |\n",
      "|    value_loss           | 286          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 143           |\n",
      "|    time_elapsed         | 225           |\n",
      "|    total_timesteps      | 292864        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025764582 |\n",
      "|    clip_fraction        | 0.00469       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0761       |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 80            |\n",
      "|    n_updates            | 1420          |\n",
      "|    policy_gradient_loss | -0.000313     |\n",
      "|    value_loss           | 252           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1299          |\n",
      "|    iterations           | 144           |\n",
      "|    time_elapsed         | 227           |\n",
      "|    total_timesteps      | 294912        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020468811 |\n",
      "|    clip_fraction        | 0.00303       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0813       |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 68.4          |\n",
      "|    n_updates            | 1430          |\n",
      "|    policy_gradient_loss | 7.93e-05      |\n",
      "|    value_loss           | 220           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1299         |\n",
      "|    iterations           | 145          |\n",
      "|    time_elapsed         | 228          |\n",
      "|    total_timesteps      | 296960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.369816e-05 |\n",
      "|    clip_fraction        | 0.0042       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0803      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 56.3         |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | 4.92e-05     |\n",
      "|    value_loss           | 190          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1299          |\n",
      "|    iterations           | 146           |\n",
      "|    time_elapsed         | 230           |\n",
      "|    total_timesteps      | 299008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010903948 |\n",
      "|    clip_fraction        | 0.00244       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0749       |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 46.1          |\n",
      "|    n_updates            | 1450          |\n",
      "|    policy_gradient_loss | 0.00421       |\n",
      "|    value_loss           | 163           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1299          |\n",
      "|    iterations           | 147           |\n",
      "|    time_elapsed         | 231           |\n",
      "|    total_timesteps      | 301056        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030170783 |\n",
      "|    clip_fraction        | 0.00527       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0565       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 35.4          |\n",
      "|    n_updates            | 1460          |\n",
      "|    policy_gradient_loss | -0.000133     |\n",
      "|    value_loss           | 137           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1299         |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 233          |\n",
      "|    total_timesteps      | 303104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.563273e-05 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0587      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.6         |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | 0.000278     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1299          |\n",
      "|    iterations           | 149           |\n",
      "|    time_elapsed         | 234           |\n",
      "|    total_timesteps      | 305152        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033456806 |\n",
      "|    clip_fraction        | 0.0041        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0494       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 20.5          |\n",
      "|    n_updates            | 1480          |\n",
      "|    policy_gradient_loss | 0.000125      |\n",
      "|    value_loss           | 93.4          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1300         |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 236          |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.744929e-05 |\n",
      "|    clip_fraction        | 0.004        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0534      |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.1         |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | -0.000152    |\n",
      "|    value_loss           | 74.7         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1300          |\n",
      "|    iterations           | 151           |\n",
      "|    time_elapsed         | 237           |\n",
      "|    total_timesteps      | 309248        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018044401 |\n",
      "|    clip_fraction        | 0.00264       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0425       |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.4           |\n",
      "|    n_updates            | 1500          |\n",
      "|    policy_gradient_loss | -0.000378     |\n",
      "|    value_loss           | 58.1          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1300         |\n",
      "|    iterations           | 152          |\n",
      "|    time_elapsed         | 239          |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002030129 |\n",
      "|    clip_fraction        | 0.00439      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0332      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.43         |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 43.8         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1300          |\n",
      "|    iterations           | 153           |\n",
      "|    time_elapsed         | 240           |\n",
      "|    total_timesteps      | 313344        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014509793 |\n",
      "|    clip_fraction        | 0.00347       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0375       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.5           |\n",
      "|    n_updates            | 1520          |\n",
      "|    policy_gradient_loss | -0.000543     |\n",
      "|    value_loss           | 31.6          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1300          |\n",
      "|    iterations           | 154           |\n",
      "|    time_elapsed         | 242           |\n",
      "|    total_timesteps      | 315392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012663877 |\n",
      "|    clip_fraction        | 0.00356       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0259       |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.886         |\n",
      "|    n_updates            | 1530          |\n",
      "|    policy_gradient_loss | -0.000641     |\n",
      "|    value_loss           | 21.6          |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 1301           |\n",
      "|    iterations           | 155            |\n",
      "|    time_elapsed         | 243            |\n",
      "|    total_timesteps      | 317440         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000103133585 |\n",
      "|    clip_fraction        | 0.00244        |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.0214        |\n",
      "|    explained_variance   | 1.19e-07       |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 0.0382         |\n",
      "|    n_updates            | 1540           |\n",
      "|    policy_gradient_loss | -6.08e-06      |\n",
      "|    value_loss           | 13.7           |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1301         |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 245          |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.470367e-05 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0186      |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00012      |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | 2.17e-05     |\n",
      "|    value_loss           | 8.02         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1301          |\n",
      "|    iterations           | 157           |\n",
      "|    time_elapsed         | 247           |\n",
      "|    total_timesteps      | 321536        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00058639667 |\n",
      "|    clip_fraction        | 0.00322       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0149       |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00267       |\n",
      "|    n_updates            | 1560          |\n",
      "|    policy_gradient_loss | -1.08e-05     |\n",
      "|    value_loss           | 5.1           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1301         |\n",
      "|    iterations           | 158          |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 323584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.438997e-05 |\n",
      "|    clip_fraction        | 0.000781     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0122      |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00295      |\n",
      "|    n_updates            | 1570         |\n",
      "|    policy_gradient_loss | -4.53e-05    |\n",
      "|    value_loss           | 2.94         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1301          |\n",
      "|    iterations           | 159           |\n",
      "|    time_elapsed         | 250           |\n",
      "|    total_timesteps      | 325632        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016322022 |\n",
      "|    clip_fraction        | 0.00156       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0105       |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000271      |\n",
      "|    n_updates            | 1580          |\n",
      "|    policy_gradient_loss | -6.81e-05     |\n",
      "|    value_loss           | 1.71          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1301         |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 251          |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.274564e-05 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00803     |\n",
      "|    explained_variance   | 3.58e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00101      |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | -8.78e-05    |\n",
      "|    value_loss           | 0.963        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1301          |\n",
      "|    iterations           | 161           |\n",
      "|    time_elapsed         | 253           |\n",
      "|    total_timesteps      | 329728        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1363958e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00713      |\n",
      "|    explained_variance   | 1.01e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000795      |\n",
      "|    n_updates            | 1600          |\n",
      "|    policy_gradient_loss | 0.000204      |\n",
      "|    value_loss           | 0.544         |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1301      |\n",
      "|    iterations           | 162       |\n",
      "|    time_elapsed         | 254       |\n",
      "|    total_timesteps      | 331776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00712  |\n",
      "|    explained_variance   | 1.25e-06  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0024    |\n",
      "|    n_updates            | 1610      |\n",
      "|    policy_gradient_loss | -2.13e-05 |\n",
      "|    value_loss           | 0.318     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1301      |\n",
      "|    iterations           | 163       |\n",
      "|    time_elapsed         | 256       |\n",
      "|    total_timesteps      | 333824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00711  |\n",
      "|    explained_variance   | 8.34e-07  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0171    |\n",
      "|    n_updates            | 1620      |\n",
      "|    policy_gradient_loss | -9.43e-05 |\n",
      "|    value_loss           | 0.182     |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1301          |\n",
      "|    iterations           | 164           |\n",
      "|    time_elapsed         | 258           |\n",
      "|    total_timesteps      | 335872        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9564399e-05 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0057       |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000126      |\n",
      "|    n_updates            | 1630          |\n",
      "|    policy_gradient_loss | -2.47e-05     |\n",
      "|    value_loss           | 0.109         |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1301      |\n",
      "|    iterations           | 165       |\n",
      "|    time_elapsed         | 259       |\n",
      "|    total_timesteps      | 337920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00527  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -9.49e-06 |\n",
      "|    n_updates            | 1640      |\n",
      "|    policy_gradient_loss | -4.02e-05 |\n",
      "|    value_loss           | 0.0625    |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1301          |\n",
      "|    iterations           | 166           |\n",
      "|    time_elapsed         | 261           |\n",
      "|    total_timesteps      | 339968        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8514648e-05 |\n",
      "|    clip_fraction        | 0.00083       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00406      |\n",
      "|    explained_variance   | 1.55e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000941      |\n",
      "|    n_updates            | 1650          |\n",
      "|    policy_gradient_loss | 0.00288       |\n",
      "|    value_loss           | 0.0362        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1301          |\n",
      "|    iterations           | 167           |\n",
      "|    time_elapsed         | 262           |\n",
      "|    total_timesteps      | 342016        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3280783e-05 |\n",
      "|    clip_fraction        | 0.00083       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00568      |\n",
      "|    explained_variance   | 9.89e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000219      |\n",
      "|    n_updates            | 1660          |\n",
      "|    policy_gradient_loss | -0.000199     |\n",
      "|    value_loss           | 0.0226        |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1301      |\n",
      "|    iterations           | 168       |\n",
      "|    time_elapsed         | 264       |\n",
      "|    total_timesteps      | 344064    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00415  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.73e-06  |\n",
      "|    n_updates            | 1670      |\n",
      "|    policy_gradient_loss | -1.64e-05 |\n",
      "|    value_loss           | 0.0137    |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 169           |\n",
      "|    time_elapsed         | 265           |\n",
      "|    total_timesteps      | 346112        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7931913e-05 |\n",
      "|    clip_fraction        | 0.000781      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00329      |\n",
      "|    explained_variance   | 1.19e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00104       |\n",
      "|    n_updates            | 1680          |\n",
      "|    policy_gradient_loss | -6.73e-06     |\n",
      "|    value_loss           | 0.00835       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1301          |\n",
      "|    iterations           | 170           |\n",
      "|    time_elapsed         | 267           |\n",
      "|    total_timesteps      | 348160        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6188332e-05 |\n",
      "|    clip_fraction        | 0.000732      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00448      |\n",
      "|    explained_variance   | 7.33e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -8.19e-05     |\n",
      "|    n_updates            | 1690          |\n",
      "|    policy_gradient_loss | -0.000237     |\n",
      "|    value_loss           | 0.00516       |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1302      |\n",
      "|    iterations           | 171       |\n",
      "|    time_elapsed         | 268       |\n",
      "|    total_timesteps      | 350208    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0036   |\n",
      "|    explained_variance   | 2.11e-05  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -4.1e-06  |\n",
      "|    n_updates            | 1700      |\n",
      "|    policy_gradient_loss | -0.000208 |\n",
      "|    value_loss           | 0.00326   |\n",
      "---------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 1302           |\n",
      "|    iterations           | 172            |\n",
      "|    time_elapsed         | 270            |\n",
      "|    total_timesteps      | 352256         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.23849895e-05 |\n",
      "|    clip_fraction        | 0.000342       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.0031        |\n",
      "|    explained_variance   | 5.91e-05       |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 0.000515       |\n",
      "|    n_updates            | 1710           |\n",
      "|    policy_gradient_loss | -0.000201      |\n",
      "|    value_loss           | 0.00203        |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 173           |\n",
      "|    time_elapsed         | 272           |\n",
      "|    total_timesteps      | 354304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010343309 |\n",
      "|    clip_fraction        | 0.00083       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0024       |\n",
      "|    explained_variance   | 1.54e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00346      |\n",
      "|    n_updates            | 1720          |\n",
      "|    policy_gradient_loss | -1.33e-05     |\n",
      "|    value_loss           | 0.00134       |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1302      |\n",
      "|    iterations           | 174       |\n",
      "|    time_elapsed         | 273       |\n",
      "|    total_timesteps      | 356352    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00232  |\n",
      "|    explained_variance   | 0.000223  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.08e-05  |\n",
      "|    n_updates            | 1730      |\n",
      "|    policy_gradient_loss | -1.97e-05 |\n",
      "|    value_loss           | 0.000856  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1302      |\n",
      "|    iterations           | 175       |\n",
      "|    time_elapsed         | 275       |\n",
      "|    total_timesteps      | 358400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00232  |\n",
      "|    explained_variance   | 3.58e-07  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.000146  |\n",
      "|    n_updates            | 1740      |\n",
      "|    policy_gradient_loss | -1.54e-05 |\n",
      "|    value_loss           | 0.000569  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 176           |\n",
      "|    time_elapsed         | 276           |\n",
      "|    total_timesteps      | 360448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00071966223 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00453      |\n",
      "|    explained_variance   | 0.000116      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.3e-05       |\n",
      "|    n_updates            | 1750          |\n",
      "|    policy_gradient_loss | -0.000553     |\n",
      "|    value_loss           | 0.000389      |\n",
      "-------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 1302     |\n",
      "|    iterations           | 177      |\n",
      "|    time_elapsed         | 278      |\n",
      "|    total_timesteps      | 362496   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00468 |\n",
      "|    explained_variance   | 0.000479 |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 3.88e-06 |\n",
      "|    n_updates            | 1760     |\n",
      "|    policy_gradient_loss | 4.42e-05 |\n",
      "|    value_loss           | 0.000261 |\n",
      "--------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 178           |\n",
      "|    time_elapsed         | 279           |\n",
      "|    total_timesteps      | 364544        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3036183e-05 |\n",
      "|    clip_fraction        | 0.00083       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00376      |\n",
      "|    explained_variance   | 9.83e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00061      |\n",
      "|    n_updates            | 1770          |\n",
      "|    policy_gradient_loss | -6.91e-05     |\n",
      "|    value_loss           | 0.00018       |\n",
      "-------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 1302     |\n",
      "|    iterations           | 179      |\n",
      "|    time_elapsed         | 281      |\n",
      "|    total_timesteps      | 366592   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00362 |\n",
      "|    explained_variance   | 0.000559 |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 5.6e-06  |\n",
      "|    n_updates            | 1780     |\n",
      "|    policy_gradient_loss | 3.63e-05 |\n",
      "|    value_loss           | 0.000127 |\n",
      "--------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1302         |\n",
      "|    iterations           | 180          |\n",
      "|    time_elapsed         | 283          |\n",
      "|    total_timesteps      | 368640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.609127e-05 |\n",
      "|    clip_fraction        | 0.000879     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00267     |\n",
      "|    explained_variance   | 0.00263      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.52e-07     |\n",
      "|    n_updates            | 1790         |\n",
      "|    policy_gradient_loss | -1.4e-05     |\n",
      "|    value_loss           | 8.88e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1302         |\n",
      "|    iterations           | 181          |\n",
      "|    time_elapsed         | 284          |\n",
      "|    total_timesteps      | 370688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007266098 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0068      |\n",
      "|    explained_variance   | 0.000302     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.59e-05     |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.000782    |\n",
      "|    value_loss           | 5.99e-05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 182           |\n",
      "|    time_elapsed         | 286           |\n",
      "|    total_timesteps      | 372736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.9604645e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00754      |\n",
      "|    explained_variance   | 0.0048        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.74e-07      |\n",
      "|    n_updates            | 1810          |\n",
      "|    policy_gradient_loss | 0.00054       |\n",
      "|    value_loss           | 4.19e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 183           |\n",
      "|    time_elapsed         | 287           |\n",
      "|    total_timesteps      | 374784        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.9604645e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00756      |\n",
      "|    explained_variance   | 0.001         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.02e-05      |\n",
      "|    n_updates            | 1820          |\n",
      "|    policy_gradient_loss | 0.000581      |\n",
      "|    value_loss           | 2.84e-05      |\n",
      "-------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 1302     |\n",
      "|    iterations           | 184      |\n",
      "|    time_elapsed         | 289      |\n",
      "|    total_timesteps      | 376832   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.0076  |\n",
      "|    explained_variance   | 3.58e-07 |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 1.96e-08 |\n",
      "|    n_updates            | 1830     |\n",
      "|    policy_gradient_loss | 2.33e-05 |\n",
      "|    value_loss           | 1.99e-05 |\n",
      "--------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1302         |\n",
      "|    iterations           | 185          |\n",
      "|    time_elapsed         | 290          |\n",
      "|    total_timesteps      | 378880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.531303e-05 |\n",
      "|    clip_fraction        | 0.00083      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00566     |\n",
      "|    explained_variance   | 0.00792      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -6.31e-07    |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -2.35e-05    |\n",
      "|    value_loss           | 1.39e-05     |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 1302           |\n",
      "|    iterations           | 186            |\n",
      "|    time_elapsed         | 292            |\n",
      "|    total_timesteps      | 380928         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000109722896 |\n",
      "|    clip_fraction        | 0.00132        |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.00399       |\n",
      "|    explained_variance   | 3.42e-05       |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 3.4e-06        |\n",
      "|    n_updates            | 1850           |\n",
      "|    policy_gradient_loss | -3.07e-05      |\n",
      "|    value_loss           | 1.05e-05       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 187           |\n",
      "|    time_elapsed         | 293           |\n",
      "|    total_timesteps      | 382976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00070955016 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0111       |\n",
      "|    explained_variance   | 0.000426      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.55e-06      |\n",
      "|    n_updates            | 1860          |\n",
      "|    policy_gradient_loss | -0.000351     |\n",
      "|    value_loss           | 7.16e-06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 188           |\n",
      "|    time_elapsed         | 295           |\n",
      "|    total_timesteps      | 385024        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -9.313226e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0115       |\n",
      "|    explained_variance   | 1.75e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.99e-05      |\n",
      "|    n_updates            | 1870          |\n",
      "|    policy_gradient_loss | 2.78e-05      |\n",
      "|    value_loss           | 5.09e-06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 189           |\n",
      "|    time_elapsed         | 297           |\n",
      "|    total_timesteps      | 387072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2394728e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0107       |\n",
      "|    explained_variance   | 0.0782        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -1.33e-06     |\n",
      "|    n_updates            | 1880          |\n",
      "|    policy_gradient_loss | 0.000218      |\n",
      "|    value_loss           | 3.23e-06      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 1302           |\n",
      "|    iterations           | 190            |\n",
      "|    time_elapsed         | 298            |\n",
      "|    total_timesteps      | 389120         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000113196176 |\n",
      "|    clip_fraction        | 0.00181        |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.00807       |\n",
      "|    explained_variance   | 0.117          |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -8.75e-07      |\n",
      "|    n_updates            | 1890           |\n",
      "|    policy_gradient_loss | -0.000163      |\n",
      "|    value_loss           | 2.23e-06       |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1302         |\n",
      "|    iterations           | 191          |\n",
      "|    time_elapsed         | 300          |\n",
      "|    total_timesteps      | 391168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.371727e-05 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00741     |\n",
      "|    explained_variance   | 0.0524       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -7.77e-07    |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -5.15e-05    |\n",
      "|    value_loss           | 1.71e-06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 192           |\n",
      "|    time_elapsed         | 301           |\n",
      "|    total_timesteps      | 393216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5640195e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00612      |\n",
      "|    explained_variance   | 0.0771        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.61e-07      |\n",
      "|    n_updates            | 1910          |\n",
      "|    policy_gradient_loss | -1.31e-05     |\n",
      "|    value_loss           | 1.14e-06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 193           |\n",
      "|    time_elapsed         | 303           |\n",
      "|    total_timesteps      | 395264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.3823455e-05 |\n",
      "|    clip_fraction        | 0.00137       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00457      |\n",
      "|    explained_variance   | 0.171         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -1.22e-07     |\n",
      "|    n_updates            | 1920          |\n",
      "|    policy_gradient_loss | -7.01e-05     |\n",
      "|    value_loss           | 8.94e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1302      |\n",
      "|    iterations           | 194       |\n",
      "|    time_elapsed         | 304       |\n",
      "|    total_timesteps      | 397312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00447  |\n",
      "|    explained_variance   | 0.0157    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -5.11e-06 |\n",
      "|    n_updates            | 1930      |\n",
      "|    policy_gradient_loss | -8.45e-06 |\n",
      "|    value_loss           | 6.68e-07  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 195           |\n",
      "|    time_elapsed         | 306           |\n",
      "|    total_timesteps      | 399360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.5569835e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00398      |\n",
      "|    explained_variance   | 0.12          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00077      |\n",
      "|    n_updates            | 1940          |\n",
      "|    policy_gradient_loss | -2.56e-05     |\n",
      "|    value_loss           | 4.18e-07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 196           |\n",
      "|    time_elapsed         | 308           |\n",
      "|    total_timesteps      | 401408        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1564454e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00311      |\n",
      "|    explained_variance   | 0.276         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -1.99e-06     |\n",
      "|    n_updates            | 1950          |\n",
      "|    policy_gradient_loss | 6.63e-06      |\n",
      "|    value_loss           | 4.34e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1302      |\n",
      "|    iterations           | 197       |\n",
      "|    time_elapsed         | 309       |\n",
      "|    total_timesteps      | 403456    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00309  |\n",
      "|    explained_variance   | 0.0424    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -1.61e-07 |\n",
      "|    n_updates            | 1960      |\n",
      "|    policy_gradient_loss | 1.81e-05  |\n",
      "|    value_loss           | 2.86e-07  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1302          |\n",
      "|    iterations           | 198           |\n",
      "|    time_elapsed         | 311           |\n",
      "|    total_timesteps      | 405504        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1108252e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00251      |\n",
      "|    explained_variance   | 0.402         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.75e-06      |\n",
      "|    n_updates            | 1970          |\n",
      "|    policy_gradient_loss | -5.97e-05     |\n",
      "|    value_loss           | 1.35e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1302      |\n",
      "|    iterations           | 199       |\n",
      "|    time_elapsed         | 312       |\n",
      "|    total_timesteps      | 407552    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00243  |\n",
      "|    explained_variance   | 0.759     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.84e-06  |\n",
      "|    n_updates            | 1980      |\n",
      "|    policy_gradient_loss | -3.19e-05 |\n",
      "|    value_loss           | 1.51e-07  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1303          |\n",
      "|    iterations           | 200           |\n",
      "|    time_elapsed         | 314           |\n",
      "|    total_timesteps      | 409600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011792092 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.002        |\n",
      "|    explained_variance   | 0.627         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.05e-06      |\n",
      "|    n_updates            | 1990          |\n",
      "|    policy_gradient_loss | 8.93e-06      |\n",
      "|    value_loss           | 1.32e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 201       |\n",
      "|    time_elapsed         | 315       |\n",
      "|    total_timesteps      | 411648    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00195  |\n",
      "|    explained_variance   | 0.884     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.07e-05  |\n",
      "|    n_updates            | 2000      |\n",
      "|    policy_gradient_loss | -3.52e-05 |\n",
      "|    value_loss           | 8.94e-08  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1303          |\n",
      "|    iterations           | 202           |\n",
      "|    time_elapsed         | 317           |\n",
      "|    total_timesteps      | 413696        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.5468885e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0014       |\n",
      "|    explained_variance   | 0.932         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -2.34e-06     |\n",
      "|    n_updates            | 2010          |\n",
      "|    policy_gradient_loss | -1.92e-05     |\n",
      "|    value_loss           | 1.07e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 203       |\n",
      "|    time_elapsed         | 319       |\n",
      "|    total_timesteps      | 415744    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00137  |\n",
      "|    explained_variance   | 0.409     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -5.5e-05  |\n",
      "|    n_updates            | 2020      |\n",
      "|    policy_gradient_loss | -2.23e-05 |\n",
      "|    value_loss           | 2.06e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 204       |\n",
      "|    time_elapsed         | 320       |\n",
      "|    total_timesteps      | 417792    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00137  |\n",
      "|    explained_variance   | 0.409     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.52e-06  |\n",
      "|    n_updates            | 2030      |\n",
      "|    policy_gradient_loss | -2.15e-05 |\n",
      "|    value_loss           | 1.97e-07  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1303          |\n",
      "|    iterations           | 205           |\n",
      "|    time_elapsed         | 322           |\n",
      "|    total_timesteps      | 419840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027995562 |\n",
      "|    clip_fraction        | 0.000879      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000818     |\n",
      "|    explained_variance   | 0.884         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.17e-07      |\n",
      "|    n_updates            | 2040          |\n",
      "|    policy_gradient_loss | -3.04e-05     |\n",
      "|    value_loss           | 4.07e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 206       |\n",
      "|    time_elapsed         | 323       |\n",
      "|    total_timesteps      | 421888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | 0.884     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.37e-06  |\n",
      "|    n_updates            | 2050      |\n",
      "|    policy_gradient_loss | 1.4e-05   |\n",
      "|    value_loss           | 2.35e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 207       |\n",
      "|    time_elapsed         | 325       |\n",
      "|    total_timesteps      | 423936    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | nan       |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.68e-07  |\n",
      "|    n_updates            | 2060      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.29e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 208       |\n",
      "|    time_elapsed         | 326       |\n",
      "|    total_timesteps      | 425984    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0         |\n",
      "|    n_updates            | 2070      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.28e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 209       |\n",
      "|    time_elapsed         | 328       |\n",
      "|    total_timesteps      | 428032    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.68e-07  |\n",
      "|    n_updates            | 2080      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 2.71e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 210       |\n",
      "|    time_elapsed         | 329       |\n",
      "|    total_timesteps      | 430080    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | 0.884     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -1.09e-06 |\n",
      "|    n_updates            | 2090      |\n",
      "|    policy_gradient_loss | -1.43e-05 |\n",
      "|    value_loss           | 3.16e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 211       |\n",
      "|    time_elapsed         | 331       |\n",
      "|    total_timesteps      | 432128    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.71e-08  |\n",
      "|    n_updates            | 2100      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 2.4e-07   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 212       |\n",
      "|    time_elapsed         | 333       |\n",
      "|    total_timesteps      | 434176    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.71e-08  |\n",
      "|    n_updates            | 2110      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.04e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 213       |\n",
      "|    time_elapsed         | 334       |\n",
      "|    total_timesteps      | 436224    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.86e-09  |\n",
      "|    n_updates            | 2120      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 1.73e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 214       |\n",
      "|    time_elapsed         | 336       |\n",
      "|    total_timesteps      | 438272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.68e-08  |\n",
      "|    n_updates            | 2130      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.58e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 215       |\n",
      "|    time_elapsed         | 337       |\n",
      "|    total_timesteps      | 440320    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.79e-06  |\n",
      "|    n_updates            | 2140      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 2.53e-07  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1303         |\n",
      "|    iterations           | 216          |\n",
      "|    time_elapsed         | 339          |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002004026 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000405    |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -1.2e-06     |\n",
      "|    n_updates            | 2150         |\n",
      "|    policy_gradient_loss | -2.74e-05    |\n",
      "|    value_loss           | 4.89e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1303      |\n",
      "|    iterations           | 217       |\n",
      "|    time_elapsed         | 340       |\n",
      "|    total_timesteps      | 444416    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.45e-09  |\n",
      "|    n_updates            | 2160      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 1.61e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 218       |\n",
      "|    time_elapsed         | 342       |\n",
      "|    total_timesteps      | 446464    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.86e-09  |\n",
      "|    n_updates            | 2170      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 7.43e-09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 219       |\n",
      "|    time_elapsed         | 343       |\n",
      "|    total_timesteps      | 448512    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.86e-09  |\n",
      "|    n_updates            | 2180      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 1.67e-06  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 220       |\n",
      "|    time_elapsed         | 345       |\n",
      "|    total_timesteps      | 450560    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.68e-08  |\n",
      "|    n_updates            | 2190      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 1.75e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 221       |\n",
      "|    time_elapsed         | 346       |\n",
      "|    total_timesteps      | 452608    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.98e-08  |\n",
      "|    n_updates            | 2200      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 4.15e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 222       |\n",
      "|    time_elapsed         | 348       |\n",
      "|    total_timesteps      | 454656    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.25e-07  |\n",
      "|    n_updates            | 2210      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 2.81e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 223       |\n",
      "|    time_elapsed         | 350       |\n",
      "|    total_timesteps      | 456704    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.71e-08  |\n",
      "|    n_updates            | 2220      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.93e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 224       |\n",
      "|    time_elapsed         | 351       |\n",
      "|    total_timesteps      | 458752    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | nan       |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.86e-07  |\n",
      "|    n_updates            | 2230      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 1.22e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 225       |\n",
      "|    time_elapsed         | 353       |\n",
      "|    total_timesteps      | 460800    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.68e-08  |\n",
      "|    n_updates            | 2240      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.17e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 226       |\n",
      "|    time_elapsed         | 354       |\n",
      "|    total_timesteps      | 462848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | nan       |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.45e-09  |\n",
      "|    n_updates            | 2250      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 2.76e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 227       |\n",
      "|    time_elapsed         | 356       |\n",
      "|    total_timesteps      | 464896    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | nan       |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.86e-09  |\n",
      "|    n_updates            | 2260      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 2.93e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 228       |\n",
      "|    time_elapsed         | 357       |\n",
      "|    total_timesteps      | 466944    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.51e-07  |\n",
      "|    n_updates            | 2270      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.36e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 229       |\n",
      "|    time_elapsed         | 359       |\n",
      "|    total_timesteps      | 468992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.13e-08  |\n",
      "|    n_updates            | 2280      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 2.99e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 230       |\n",
      "|    time_elapsed         | 361       |\n",
      "|    total_timesteps      | 471040    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.86e-09  |\n",
      "|    n_updates            | 2290      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 2.95e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 231       |\n",
      "|    time_elapsed         | 362       |\n",
      "|    total_timesteps      | 473088    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0         |\n",
      "|    n_updates            | 2300      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 7.43e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 232       |\n",
      "|    time_elapsed         | 364       |\n",
      "|    total_timesteps      | 475136    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.68e-08  |\n",
      "|    n_updates            | 2310      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 5.2e-08   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 233       |\n",
      "|    time_elapsed         | 365       |\n",
      "|    total_timesteps      | 477184    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.86e-09  |\n",
      "|    n_updates            | 2320      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.28e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 234       |\n",
      "|    time_elapsed         | 367       |\n",
      "|    total_timesteps      | 479232    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.98e-08  |\n",
      "|    n_updates            | 2330      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 1.91e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 235       |\n",
      "|    time_elapsed         | 368       |\n",
      "|    total_timesteps      | 481280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.15e-07  |\n",
      "|    n_updates            | 2340      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 5.76e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 236       |\n",
      "|    time_elapsed         | 370       |\n",
      "|    total_timesteps      | 483328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | nan       |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.66e-08  |\n",
      "|    n_updates            | 2350      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 1.49e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 237       |\n",
      "|    time_elapsed         | 372       |\n",
      "|    total_timesteps      | 485376    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.98e-08  |\n",
      "|    n_updates            | 2360      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.47e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 238       |\n",
      "|    time_elapsed         | 373       |\n",
      "|    total_timesteps      | 487424    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.68e-08  |\n",
      "|    n_updates            | 2370      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 2.75e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 239       |\n",
      "|    time_elapsed         | 375       |\n",
      "|    total_timesteps      | 489472    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.45e-09  |\n",
      "|    n_updates            | 2380      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 2.52e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 240       |\n",
      "|    time_elapsed         | 376       |\n",
      "|    total_timesteps      | 491520    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.65e-07  |\n",
      "|    n_updates            | 2390      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.37e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 241       |\n",
      "|    time_elapsed         | 378       |\n",
      "|    total_timesteps      | 493568    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 0.543     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.46e-06  |\n",
      "|    n_updates            | 2400      |\n",
      "|    policy_gradient_loss | 1.7e-06   |\n",
      "|    value_loss           | 2.61e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 242       |\n",
      "|    time_elapsed         | 379       |\n",
      "|    total_timesteps      | 495616    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.98e-08  |\n",
      "|    n_updates            | 2410      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 3.18e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1304      |\n",
      "|    iterations           | 243       |\n",
      "|    time_elapsed         | 381       |\n",
      "|    total_timesteps      | 497664    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0         |\n",
      "|    n_updates            | 2420      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 1.11e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1305      |\n",
      "|    iterations           | 244       |\n",
      "|    time_elapsed         | 382       |\n",
      "|    total_timesteps      | 499712    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0         |\n",
      "|    n_updates            | 2430      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 0         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1305      |\n",
      "|    iterations           | 245       |\n",
      "|    time_elapsed         | 384       |\n",
      "|    total_timesteps      | 501760    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000368 |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0         |\n",
      "|    n_updates            | 2440      |\n",
      "|    policy_gradient_loss | 0         |\n",
      "|    value_loss           | 0         |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN, SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "vec_env = make_vec_env(SudokuEnv, n_envs=1, \n",
    "                       env_kwargs=dict(size=4))\n",
    "\n",
    "# Train the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1).learn(500000)\n",
    "# 10000 -> 40\n",
    "# 100000 -> 480\n",
    "# 500000 -> 2440 (약 6분 소요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egtechlab/anaconda3/envs/mujoco_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 and action=[39]\n",
      "obs= [[0 0 0 0 0 0 0 0 0 4 1 0 0 0 0 0]] reward= [-10.] done= [False]\n",
      "Step 200 and action=[39]\n",
      "obs= [[0 0 0 0 0 0 0 0 0 4 1 0 0 0 0 0]] reward= [-10.] done= [False]\n",
      "Step 300 and action=[39]\n",
      "obs= [[0 0 0 0 0 0 0 0 0 4 1 0 0 3 0 0]] reward= [-10.] done= [False]\n",
      "Step 400 and action=[39]\n",
      "obs= [[0 0 0 0 0 0 0 0 0 4 1 0 0 3 0 0]] reward= [-10.] done= [False]\n",
      "Step 500 and action=[39]\n",
      "obs= [[0 0 0 0 0 0 0 0 0 4 1 0 0 3 0 0]] reward= [-10.] done= [False]\n",
      "Step 600 and action=[39]\n",
      "obs= [[0 0 0 0 0 0 0 0 0 4 1 0 0 3 0 0]] reward= [-10.] done= [False]\n",
      "Step 700 and action=[39]\n",
      "obs= [[0 0 0 0 0 0 2 0 0 4 1 0 0 3 4 0]] reward= [-10.] done= [False]\n",
      "Step 800 and action=[39]\n",
      "obs= [[0 0 0 0 0 0 2 0 0 4 1 0 0 3 4 0]] reward= [-10.] done= [False]\n",
      "Step 900 and action=[39]\n",
      "obs= [[1 0 0 0 0 0 2 0 0 4 1 0 0 3 4 0]] reward= [-10.] done= [False]\n",
      "Step 1000 and action=[39]\n",
      "obs= [[1 0 0 0 4 0 2 0 2 4 1 0 0 3 4 0]] reward= [-10.] done= [False]\n",
      "Step 1100 and action=[39]\n",
      "obs= [[1 0 0 0 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 1200 and action=[39]\n",
      "obs= [[1 0 0 0 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 1300 and action=[39]\n",
      "obs= [[1 0 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 1400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 1500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 1600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 1700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 1800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 1900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 2000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 2100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 2200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 2300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 2400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 2500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 2600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 0 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 2700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 2800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 2900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 3000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 3100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 3200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 3300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 3400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 3500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 3600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 3700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 3800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 3900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 4000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 4100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 4200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 4300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 4400 and action=[11]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 4500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 4600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 4700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 4800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 4900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 5000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 5100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 5200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 5300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 5400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 5500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 5600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 5700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 5800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 5900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 6000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 6100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 6200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 6300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 6400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 6500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 6600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 6700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 6800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 6900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 7000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 7100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 7200 and action=[50]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 7300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 7400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 7500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 7600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 7700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 7800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 7900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 8000 and action=[62]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 8100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 8200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 8300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 8400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 8500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 8600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 8700 and action=[11]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 8800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 8900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 9000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 9100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 9200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 9300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 9400 and action=[35]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 9500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 9600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 9700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 9800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 9900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 10000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 10100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 10200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 10300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 10400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 10500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 10600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 10700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 10800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 10900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 11000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 11100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 11200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 11300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 11400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 11500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 11600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 11700 and action=[52]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 11800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 11900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 12000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 12100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 12200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 12300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 12400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 12500 and action=[52]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 12600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 12700 and action=[32]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 12800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 12900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 13000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 13100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 13200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 13300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 13400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 13500 and action=[29]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 13600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 13700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 13800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 13900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 14000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 14100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 14200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 14300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 14400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 14500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 14600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 14700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 14800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 14900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 15000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 15100 and action=[56]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 15200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 15300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 15400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 15500 and action=[48]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 15600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 15700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 15800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 15900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 16000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 16100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 16200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 16300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 16400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 16500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 16600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 16700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 16800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 16900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 17000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 17100 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 17200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 17300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 17400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 17500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 17600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 17700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 17800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 17900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 18000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 18100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 18200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 18300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 18400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 18500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 18600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 18700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 18800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 18900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 19000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 19100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 19200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 19300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 19400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 19500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 19600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 19700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 19800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 19900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 20000 and action=[30]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 20100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 20200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 20300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 20400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 20500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 20600 and action=[22]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 20700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 20800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 20900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 21000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 21100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 21200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 21300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 21400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 21500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 21600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 21700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 21800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 21900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 22000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 22100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 22200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 22300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 22400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 22500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 22600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 22700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 22800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 22900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 23000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 23100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 23200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 23300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 23400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 23500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 23600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 23700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 23800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 23900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 24000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 24100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 24200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 24300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 24400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 24500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 24600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 24700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 24800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 24900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 25000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 25100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 25200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 25300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 25400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 25500 and action=[58]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 25600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 25700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 25800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 25900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 26000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 26100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 26200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 26300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 26400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 26500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 26600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 26700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 26800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 26900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 27000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 27100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 27200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 27300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 27400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 27500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 27600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 27700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 27800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 27900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 28000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 28100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 28200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 28300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 28400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 28500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 28600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 28700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 28800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 28900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 29000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 29100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 29200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 29300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 29400 and action=[24]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 29500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 29600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 29700 and action=[63]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 29800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 29900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 30000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 30100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 30200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 30300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 30400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 30500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 30600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 30700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 30800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 30900 and action=[47]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 31000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 31100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 31200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 31300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 31400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 31500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 31600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 31700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 31800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 31900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 32000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 32100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 32200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 32300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 32400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 32500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 32600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 32700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 32800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 32900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 33000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 33100 and action=[3]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 33200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 33300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 33400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 33500 and action=[53]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 33600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 33700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 33800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 33900 and action=[3]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 34000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 34100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 34200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 34300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 34400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 34500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 34600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 34700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 34800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 34900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 35000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 35100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 35200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 35300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 35400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 35500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 35600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 35700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 35800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 35900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 36000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 36100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 36200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 36300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 36400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 36500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 36600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 36700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 36800 and action=[34]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 36900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 37000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 37100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 37200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 37300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 37400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 37500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 37600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 37700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 37800 and action=[1]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 37900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 38000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 38100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 38200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 38300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 38400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 38500 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 38600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 38700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 38800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 38900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 39000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 39100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 39200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 39300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 39400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 39500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 39600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 39700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 39800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 39900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 40000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 40100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 40200 and action=[40]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 40300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 40400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 40500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 40600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 40700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 40800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 40900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 41000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 41100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 41200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 41300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 41400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 41500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 41600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 41700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 41800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 41900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 42000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 42100 and action=[47]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 42200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 42300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 42400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 42500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 42600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 42700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 42800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 42900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 43000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 43100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 43200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 43300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 43400 and action=[23]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 43500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 43600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 43700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 43800 and action=[26]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 43900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 44000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 44100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 44200 and action=[31]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 44300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 44400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 44500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 44600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 44700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 44800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 44900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 45000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 45100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 45200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 45300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 45400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 45500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 45600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 45700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 45800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 45900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 46000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 46100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 46200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 46300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 46400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 46500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 46600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 46700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 46800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 46900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 47000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 47100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 47200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 47300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 47400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 47500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 47600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 47700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 47800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 47900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 48000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 48100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 48200 and action=[11]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 48300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 48400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 48500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 48600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 48700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 48800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 48900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 49000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 49100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 49200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 49300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 49400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 49500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 49600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 49700 and action=[26]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 49800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 49900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 50000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 50100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 50200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 50300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 50400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 50500 and action=[53]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 50600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 50700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 50800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 50900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 51000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 51100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 51200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 51300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 51400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 51500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 51600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 51700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 51800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 51900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 52000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 52100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 52200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 52300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 52400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 52500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 52600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 52700 and action=[60]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 52800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 52900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 53000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 53100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 53200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 53300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 53400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 53500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 53600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 53700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 53800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 53900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 54000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 54100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 54200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 54300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 54400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 54500 and action=[32]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 54600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 54700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 54800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 54900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 55000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 55100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 55200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 55300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 55400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 55500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 55600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 55700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 55800 and action=[40]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 55900 and action=[49]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 56000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 56100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 56200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 56300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 56400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 56500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 56600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 56700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 56800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 56900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 57000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 57100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 57200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 57300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 57400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 57500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 57600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 57700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 57800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 57900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 58000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 58100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 58200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 58300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 58400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 58500 and action=[30]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 58600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 58700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 58800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 58900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 59000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 59100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 59200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 59300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 59400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 59500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 59600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 59700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 59800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 59900 and action=[23]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 60000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 60100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 60200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 60300 and action=[20]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 60400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 60500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 60600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 60700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 60800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 60900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 61000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 61100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 61200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 61300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 61400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 61500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 61600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 61700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 61800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 61900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 62000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 62100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 62200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 62300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 62400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 62500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 62600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 62700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 62800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 62900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 63000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 63100 and action=[22]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 63200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 63300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 63400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 63500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 63600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 63700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 63800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 63900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 64000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 64100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 64200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 64300 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 64400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 64500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 64600 and action=[11]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 64700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 64800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 64900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 65000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 65100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 65200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 65300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 65400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 65500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 65600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 65700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 65800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 65900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 66000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 66100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 66200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 66300 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 66400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 66500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 66600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 66700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 66800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 66900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 67000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 67100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 67200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 67300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 67400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 67500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 67600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 67700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 67800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 67900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 68000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 68100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 68200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 68300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 68400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 68500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 68600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 68700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 68800 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 68900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 69000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 69100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 69200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 69300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 69400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 69500 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 69600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 69700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 69800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 69900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 70000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 70100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 70200 and action=[35]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 70300 and action=[49]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 70400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 70500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 70600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 70700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 70800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 70900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 71000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 71100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 71200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 71300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 71400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 71500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 71600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 71700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 71800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 71900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 72000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 72100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 72200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 72300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 72400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 72500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 72600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 72700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 72800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 72900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 73000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 73100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 73200 and action=[41]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 73300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 73400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 73500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 73600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 73700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 73800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 73900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 74000 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 74100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 74200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 74300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 74400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 74500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 74600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 74700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 74800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 74900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 75000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 75100 and action=[24]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 75200 and action=[10]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 75300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 75400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 75500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 75600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 75700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 75800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 75900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 76000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 76100 and action=[32]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 76200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 76300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 76400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 76500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 76600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 76700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 76800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 76900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 77000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 77100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 77200 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 77300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 77400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 77500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 77600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 77700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 77800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 77900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 78000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 78100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 78200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 78300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 78400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 78500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 78600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 78700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 78800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 78900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 79000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 79100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 79200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 79300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 79400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 79500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 79600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 79700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 79800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 79900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 80000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 80100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 80200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 80300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 80400 and action=[11]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 80500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 80600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 80700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 80800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 80900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 81000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 81100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 81200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 81300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 81400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 81500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 81600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 81700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 81800 and action=[50]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 81900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 82000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 82100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 82200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 82300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 82400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 82500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 82600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 82700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 82800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 82900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 83000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 83100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 83200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 83300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 83400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 83500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 83600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 83700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 83800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 83900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 84000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 84100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 84200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 84300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 84400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 84500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 84600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 84700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 84800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 84900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 85000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 85100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 85200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 85300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 85400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 85500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 85600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 85700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 85800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 85900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 86000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 86100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 86200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 86300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 86400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 86500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 86600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 86700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 86800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 86900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 87000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 87100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 87200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 87300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 87400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 87500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 87600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 87700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 87800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 87900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 88000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 88100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 88200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 88300 and action=[47]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 88400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 88500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 88600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 88700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 88800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 88900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 89000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 89100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 89200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 89300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 89400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 89500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 89600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 89700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 89800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 89900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 90000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 90100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 90200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 90300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 90400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 90500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 90600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 90700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 90800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 90900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 91000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 91100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 91200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 91300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 91400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 91500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 91600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 91700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 91800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 91900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 92000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 92100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 92200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 92300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 92400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 92500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 92600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 92700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 92800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 92900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 93000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 93100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 93200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 93300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 93400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 93500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 93600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 93700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 93800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 93900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 94000 and action=[5]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 94100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 94200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 94300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 94400 and action=[53]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 94500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 94600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 94700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 94800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 94900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 95000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 95100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 95200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 95300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 95400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 95500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 95600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 95700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 95800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 95900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 96000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 96100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 96200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 96300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 96400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 96500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 96600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 96700 and action=[58]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 96800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 96900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 97000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 97100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 97200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 97300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 97400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 97500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 97600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 97700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 97800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 97900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 98000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 98100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 98200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 98300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 98400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 98500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 98600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 98700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 98800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 98900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 99000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 99100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 99200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 99300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 99400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 99500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 99600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 99700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 99800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 99900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 100000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 100100 and action=[59]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 100200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 100300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 100400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 100500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 100600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 100700 and action=[30]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 100800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 100900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 101000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 101100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 101200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 101300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 101400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 101500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 101600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 101700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 101800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 101900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 102000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 102100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 102200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 102300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 102400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 102500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 102600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 102700 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 102800 and action=[31]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 102900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 103000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 103100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 103200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 103300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 103400 and action=[55]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 103500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 103600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 103700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 103800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 103900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 104000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 104100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 104200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 104300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 104400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 104500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 104600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 104700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 104800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 104900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 105000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 105100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 105200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 105300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 105400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 105500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 105600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 105700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 105800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 105900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 106000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 106100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 106200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 106300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 106400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 106500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 106600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 106700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 106800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 106900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 107000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 107100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 107200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 107300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 107400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 107500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 107600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 107700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 107800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 107900 and action=[21]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 108000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 108100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 108200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 108300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 108400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 108500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 108600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 108700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 108800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 108900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 109000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 109100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 109200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 109300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 109400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 109500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 109600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 109700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 109800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 109900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 110000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 110100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 110200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 110300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 110400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 110500 and action=[27]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 110600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 110700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 110800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 110900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 111000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 111100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 111200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 111300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 111400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 111500 and action=[53]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 111600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 111700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 111800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 111900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 112000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 112100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 112200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 112300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 112400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 112500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 112600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 112700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 112800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 112900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 113000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 113100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 113200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 113300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 113400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 113500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 113600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 113700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 113800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 113900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 114000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 114100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 114200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 114300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 114400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 114500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 114600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 114700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 114800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 114900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 115000 and action=[50]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 115100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 115200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 115300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 115400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 115500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 115600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 115700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 115800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 115900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 116000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 116100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 116200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 116300 and action=[24]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 116400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 116500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 116600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 116700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 116800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 116900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 117000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 117100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 117200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 117300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 117400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 117500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 117600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 117700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 117800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 117900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 118000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 118100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 118200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 118300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 118400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 118500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 118600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 118700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 118800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 118900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 119000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 119100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 119200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 119300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 119400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 119500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 119600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 119700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 119800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 119900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 120000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 120100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 120200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 120300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 120400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 120500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 120600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 120700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 120800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 120900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 121000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 121100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 121200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 121300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 121400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 121500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 121600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 121700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 121800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 121900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 122000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 122100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 122200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 122300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 122400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 122500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 122600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 122700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 122800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 122900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 123000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 123100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 123200 and action=[63]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 123300 and action=[10]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 123400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 123500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 123600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 123700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 123800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 123900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 124000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 124100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 124200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 124300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 124400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 124500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 124600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 124700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 124800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 124900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 125000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 125100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 125200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 125300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 125400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 125500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 125600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 125700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 125800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 125900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 126000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 126100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 126200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 126300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 126400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 126500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 126600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 126700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 126800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 126900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 127000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 127100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 127200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 127300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 127400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 127500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 127600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 127700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 127800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 127900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 128000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 128100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 128200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 128300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 128400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 128500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 128600 and action=[63]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 128700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 128800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 128900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 129000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 129100 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 129200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 129300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 129400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 129500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 129600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 129700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 129800 and action=[40]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 129900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 130000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 130100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 130200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 130300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 130400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 130500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 130600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 130700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 130800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 130900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 131000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 131100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 131200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 131300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 131400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 131500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 131600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 131700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 131800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 131900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 132000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 132100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 132200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 132300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 132400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 132500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 132600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 132700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 132800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 132900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 133000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 133100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 133200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 133300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 133400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 133500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 133600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 133700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 133800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 133900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 134000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 134100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 134200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 134300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 134400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 134500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 134600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 134700 and action=[47]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 134800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 134900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 135000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 135100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 135200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 135300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 135400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 135500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 135600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 135700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 135800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 135900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 136000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 136100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 136200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 136300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 136400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 136500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 136600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 136700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 136800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 136900 and action=[24]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 137000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 137100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 137200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 137300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 137400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 137500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 137600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 137700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 137800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 137900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 138000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 138100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 138200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 138300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 138400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 138500 and action=[1]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 138600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 138700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 138800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 138900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 139000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 139100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 139200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 139300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 139400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 139500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 139600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 139700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 139800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 139900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 140000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 140100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 140200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 140300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 140400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 140500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 140600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 140700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 140800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 140900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 141000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 141100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 141200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 141300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 141400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 141500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 141600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 141700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 141800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 141900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 142000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 142100 and action=[23]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 142200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 142300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 142400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 142500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 142600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 142700 and action=[7]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 142800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 142900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 143000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 143100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 143200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 143300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 143400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 143500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 143600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 143700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 143800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 143900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 144000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 144100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 144200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 144300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 144400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 144500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 144600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 144700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 144800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 144900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 145000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 145100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 145200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 145300 and action=[35]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 145400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 145500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 145600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 145700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 145800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 145900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 146000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 146100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 146200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 146300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 146400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 146500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 146600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 146700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 146800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 146900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 147000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 147100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 147200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 147300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 147400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 147500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 147600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 147700 and action=[62]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 147800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 147900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 148000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 148100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 148200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 148300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 148400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 148500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 148600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 148700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 148800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 148900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 149000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 149100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 149200 and action=[17]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 149300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 149400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 149500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 149600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 149700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 149800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 149900 and action=[11]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 150000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 150100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 150200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 150300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 150400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 150500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 150600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 150700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 150800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 150900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 151000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 151100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 151200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 151300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 151400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 151500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 151600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 151700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 151800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 151900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 152000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 152100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 152200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 152300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 152400 and action=[23]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 152500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 152600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 152700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 152800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 152900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 153000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 153100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 153200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 153300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 153400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 153500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 153600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 153700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 153800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 153900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 154000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 154100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 154200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 154300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 154400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 154500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 154600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 154700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 154800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 154900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 155000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 155100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 155200 and action=[30]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 155300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 155400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 155500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 155600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 155700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 155800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 155900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 156000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 156100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 156200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 156300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 156400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 156500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 156600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 156700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 156800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 156900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 157000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 157100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 157200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 157300 and action=[31]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 157400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 157500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 157600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 157700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 157800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 157900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 158000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 158100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 158200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 158300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 158400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 158500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 158600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 158700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 158800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 158900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 159000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 159100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 159200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 159300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 159400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 159500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 159600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 159700 and action=[18]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 159800 and action=[26]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 159900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 160000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 160100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 160200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 160300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 160400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 160500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 160600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 160700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 160800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 160900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 161000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 161100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 161200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 161300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 161400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 161500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 161600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 161700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 161800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 161900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 162000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 162100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 162200 and action=[50]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 162300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 162400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 162500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 162600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 162700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 162800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 162900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 163000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 163100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 163200 and action=[4]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 163300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 163400 and action=[60]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 163500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 163600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 163700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 163800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 163900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 164000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 164100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 164200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 164300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 164400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 164500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 164600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 164700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 164800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 164900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 165000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 165100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 165200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 165300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 165400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 165500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 165600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 165700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 165800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 165900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 166000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 166100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 166200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 166300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 166400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 166500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 166600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 166700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 166800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 166900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 167000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 167100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 167200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 167300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 167400 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 167500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 167600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 167700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 167800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 167900 and action=[22]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 168000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 168100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 168200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 168300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 168400 and action=[61]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 168500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 168600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 168700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 168800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 168900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 169000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 169100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 169200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 169300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 169400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 169500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 169600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 169700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 169800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 169900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 170000 and action=[18]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 170100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 170200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 170300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 170400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 170500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 170600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 170700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 170800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 170900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 171000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 171100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 171200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 171300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 171400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 171500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 171600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 171700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 171800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 171900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 172000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 172100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 172200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 172300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 172400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 172500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 172600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 172700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 172800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 172900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 173000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 173100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 173200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 173300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 173400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 173500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 173600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 173700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 173800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 173900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 174000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 174100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 174200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 174300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 174400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 174500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 174600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 174700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 174800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 174900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 175000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 175100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 175200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 175300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 175400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 175500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 175600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 175700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 175800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 175900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 176000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 176100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 176200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 176300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 176400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 176500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 176600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 176700 and action=[62]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 176800 and action=[23]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 176900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 177000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 177100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 177200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 177300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 177400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 177500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 177600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 177700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 177800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 177900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 178000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 178100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 178200 and action=[12]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 178300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 178400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 178500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 178600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 178700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 178800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 178900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 179000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 179100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 179200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 179300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 179400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 179500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 179600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 179700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 179800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 179900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 180000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 180100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 180200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 180300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 180400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 180500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 180600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 180700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 180800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 180900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 181000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 181100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 181200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 181300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 181400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 181500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 181600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 181700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 181800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 181900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 182000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 182100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 182200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 182300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 182400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 182500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 182600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 182700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 182800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 182900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 183000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 183100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 183200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 183300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 183400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 183500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 183600 and action=[51]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 183700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 183800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 183900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 184000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 184100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 184200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 184300 and action=[23]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 184400 and action=[55]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 184500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 184600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 184700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 184800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 184900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 185000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 185100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 185200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 185300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 185400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 185500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 185600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 185700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 185800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 185900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 186000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 186100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 186200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 186300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 186400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 186500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 186600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 186700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 186800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 186900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 187000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 187100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 187200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 187300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 187400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 187500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 187600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 187700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 187800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 187900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 188000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 188100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 188200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 188300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 188400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 188500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 188600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 188700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 188800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 188900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 189000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 189100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 189200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 189300 and action=[37]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 189400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 189500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 189600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 189700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 189800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 189900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 190000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 190100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 190200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 190300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 190400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 190500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 190600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 190700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 190800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 190900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 191000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 191100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 191200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 191300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 191400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 191500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 191600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 191700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 191800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 191900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 192000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 192100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 192200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 192300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 192400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 192500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 192600 and action=[35]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 192700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 192800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 192900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 193000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 193100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 193200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 193300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 193400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 193500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 193600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 193700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 193800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 193900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 194000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 194100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 194200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 194300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 194400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 194500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 194600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 194700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 194800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 194900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 195000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 195100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 195200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 195300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 195400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 195500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 195600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 195700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 195800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 195900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 196000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 196100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 196200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 196300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 196400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 196500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 196600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 196700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 196800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 196900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 197000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 197100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 197200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 197300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 197400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 197500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 197600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 197700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 197800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 197900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 198000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 198100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 198200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 198300 and action=[31]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 198400 and action=[14]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 198500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 198600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 198700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 198800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 198900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 199000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 199100 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 199200 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 199300 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 199400 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 199500 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 199600 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 199700 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 199800 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 199900 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n",
      "Step 200000 and action=[39]\n",
      "obs= [[1 2 0 3 4 0 2 1 2 4 1 0 0 3 4 2]] reward= [-10.] done= [False]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "# using the vecenv\n",
    "obs = vec_env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "while not done:\n",
    "    action, _ = model.predict(obs) #, deterministic=True)\n",
    "    step += 1\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    \n",
    "    vec_env.render()\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step} and action={action}\")\n",
    "        print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    if done:        \n",
    "        print(\"Goal reached!\", \"reward=\", reward, \"step=\", step)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
